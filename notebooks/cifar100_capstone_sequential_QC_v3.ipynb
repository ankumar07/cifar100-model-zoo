{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb333fd9",
   "metadata": {},
   "source": [
    "\n",
    "# Capstone — CIFAR‑100 (QC'd, Sequential)  \n",
    "**Models:** ResNet‑18 • WideResNet‑28×10 • ConvNeXt‑Tiny • ViT‑Tiny • ViT‑Hybrid (ResNet‑26 + ViT Small)  \n",
    "\n",
    "This notebook is **QC'd** and follows **capstone best practices**. It trains and compares multiple architectures on **CIFAR‑100**, producing:\n",
    "- Metrics: **Top‑1 / Top‑5**, loss (val/test)\n",
    "- **Confusion matrix**, **per‑class accuracy** (with class names)\n",
    "- **Calibration**: **ECE** (raw) and **ECE after temperature scaling** with **reliability diagrams**\n",
    "- **Best checkpoint** per model (by validation Top‑1)\n",
    "- **Per‑epoch CSV logs**, plots, and **aggregate summary**\n",
    "- **Early stopping** with patience/min_delta/min_epochs and **target Top‑1** thresholds\n",
    "- **Strong augmentations**, optional **Mixup/CutMix**, **EMA**, **AMP**, **channels‑last**, grad accumulation\n",
    "- Optional **TensorBoard/W&B** logging and **one‑click ZIP export**\n",
    "\n",
    "> Runs **sequentially with a dedicated cell for each model** so you can execute them one by one or all together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209555de",
   "metadata": {},
   "source": [
    "## 0) (Optional) Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ec41e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install torch torchvision timm scikit-learn matplotlib pandas tensorboard wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05119a2f",
   "metadata": {},
   "source": [
    "## 1) Environment & Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52086007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akugupta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.10 (tags/v3.12.10:0cc8128, Apr  8 2025, 12:21:36) [MSC v.1943 64 bit (AMD64)]\n",
      "OS: Windows-11-10.0.26100-SP0\n",
      "PyTorch: 2.5.1+cu121\n",
      "Torchvision: 0.20.1+cu121\n",
      "timm: 1.0.22\n",
      "CUDA available: True\n",
      "GPU count: 1\n",
      "GPU name[0]: NVIDIA RTX 2000 Ada Generation Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, math, json, random, time, sys, platform, copy, shutil\n",
    "from datetime import datetime\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import timm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"OS:\", platform.platform())\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"Torchvision:\", torchvision.__version__)\n",
    "print(\"timm:\", timm.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU count:\", torch.cuda.device_count())\n",
    "    print(\"GPU name[0]:\", torch.cuda.get_device_name(0))\n",
    "    torch.set_float32_matmul_precision('high')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41248d61",
   "metadata": {},
   "source": [
    "## 2) Utilities: Seeding, Metrics, ECE, Temperature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9078f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalization stats\n",
    "CIFAR_MEAN = [0.5071, 0.4867, 0.4408]\n",
    "CIFAR_STD  = [0.2675, 0.2565, 0.2761]\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "def seed_everything(seed: int = 42, deterministic: bool = False):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    if deterministic:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    else:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def ensure_dir(path: str): os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def topk_accuracy(logits: torch.Tensor, target: torch.Tensor, ks=(1,5)) -> List[float]:\n",
    "    maxk = max(ks)\n",
    "    with torch.no_grad():\n",
    "        _, pred = logits.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "        res = []\n",
    "        for k in ks:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "            res.append((correct_k / target.size(0)).item())\n",
    "        return res\n",
    "\n",
    "def one_hot(targets: torch.Tensor, num_classes: int, smoothing: float = 0.0) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        y = torch.zeros(targets.size(0), num_classes, device=targets.device)\n",
    "        y.scatter_(1, targets.view(-1,1), 1.0)\n",
    "        if smoothing > 0.0:\n",
    "            y = y * (1.0 - smoothing) + smoothing / num_classes\n",
    "    return y\n",
    "\n",
    "def soft_cross_entropy(logits: torch.Tensor, soft_targets: torch.Tensor) -> torch.Tensor:\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    return -(soft_targets * log_probs).sum(dim=1).mean()\n",
    "\n",
    "def compute_ece(confidences: np.ndarray, correctness: np.ndarray, n_bins: int = 15) -> Tuple[float, Dict]:\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    stats = {\"bin_left\": [], \"bin_right\": [], \"bin_acc\": [], \"bin_conf\": [], \"bin_count\": []}\n",
    "    N = len(confidences)\n",
    "    for i in range(n_bins):\n",
    "        l, r = bins[i], bins[i+1]\n",
    "        idx = (confidences > l) & (confidences <= r) if i > 0 else (confidences >= l) & (confidences <= r)\n",
    "        if idx.sum() > 0:\n",
    "            acc = correctness[idx].mean(); conf = confidences[idx].mean()\n",
    "            ece += (idx.sum() / N) * abs(acc - conf)\n",
    "            stats[\"bin_left\"].append(float(l)); stats[\"bin_right\"].append(float(r))\n",
    "            stats[\"bin_acc\"].append(float(acc)); stats[\"bin_conf\"].append(float(conf))\n",
    "            stats[\"bin_count\"].append(int(idx.sum()))\n",
    "        else:\n",
    "            stats[\"bin_left\"].append(float(l)); stats[\"bin_right\"].append(float(r))\n",
    "            stats[\"bin_acc\"].append(0.0); stats[\"bin_conf\"].append(0.0); stats[\"bin_count\"].append(0)\n",
    "    return float(ece), stats\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_logits(model: nn.Module, dl: DataLoader, device, use_channels_last: bool = True) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    model.eval()\n",
    "    logits_all, targets_all = [], []\n",
    "    for images, targets in dl:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        if use_channels_last and torch.cuda.is_available():\n",
    "            images = images.to(memory_format=torch.channels_last)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        logits = model(images)\n",
    "        logits_all.append(logits.detach().cpu())\n",
    "        targets_all.append(targets.detach().cpu())\n",
    "    return torch.cat(logits_all,0), torch.cat(targets_all,0)\n",
    "\n",
    "def fit_temperature(logits: torch.Tensor, labels: torch.Tensor, max_iter: int = 200) -> float:\n",
    "    T = torch.ones(1, requires_grad=True)\n",
    "    optimizer = torch.optim.LBFGS([T], lr=0.5, max_iter=50)\n",
    "    nll_criterion = nn.CrossEntropyLoss()\n",
    "    def _eval():\n",
    "        optimizer.zero_grad()\n",
    "        loss = nll_criterion(logits / T.clamp_min(1e-6), labels)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    for _ in range(max_iter):\n",
    "        optimizer.step(_eval)\n",
    "    return float(T.detach().clamp_min(1e-6).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdde01c",
   "metadata": {},
   "source": [
    "## 3) WideResNet‑28×10 (reference implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c44e8848",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride, drop_rate=0.0):\n",
    "        super().__init__()\n",
    "        self.equalInOut = (in_planes == out_planes)\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.drop_rate = drop_rate\n",
    "        self.shortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1,\n",
    "                                                            stride=stride, padding=0, bias=False) or None\n",
    "    def forward(self, x):\n",
    "        out = self.relu1(self.bn1(x))\n",
    "        out = self.conv1(out)\n",
    "        out = self.relu2(self.bn2(out))\n",
    "        if self.drop_rate > 0.0:\n",
    "            out = F.dropout(out, p=self.drop_rate, training=self.training)\n",
    "        out = self.conv2(out)\n",
    "        return out + (x if self.equalInOut else self.shortcut(x))\n",
    "\n",
    "class NetworkBlock(nn.Module):\n",
    "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, drop_rate):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(nb_layers):\n",
    "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, drop_rate))\n",
    "        self.layer = nn.Sequential(*layers)\n",
    "    def forward(self, x): return self.layer(x)\n",
    "\n",
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, depth=28, widen_factor=10, num_classes=100, drop_rate=0.3):\n",
    "        super().__init__()\n",
    "        assert ((depth - 4) % 6 == 0), \"Depth should be 6n+4\"\n",
    "        n = (depth - 4) // 6; k = widen_factor\n",
    "        nStages = [16, 16*k, 32*k, 64*k]\n",
    "        self.conv1 = nn.Conv2d(3, nStages[0], kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.block1 = NetworkBlock(n, nStages[0], nStages[1], BasicBlock, 1, drop_rate)\n",
    "        self.block2 = NetworkBlock(n, nStages[1], nStages[2], BasicBlock, 2, drop_rate)\n",
    "        self.block3 = NetworkBlock(n, nStages[2], nStages[3], BasicBlock, 2, drop_rate)\n",
    "        self.bn = nn.BatchNorm2d(nStages[3]); self.relu = nn.ReLU(inplace=True)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1,1)); self.fc = nn.Linear(nStages[3], num_classes)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "            elif isinstance(m, nn.BatchNorm2d): nn.init.constant_(m.weight, 1); nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear): nn.init.xavier_normal_(m.weight); nn.init.constant_(m.bias, 0)\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x); out = self.block1(out); out = self.block2(out); out = self.block3(out)\n",
    "        out = self.relu(self.bn(out)); out = self.pool(out).flatten(1)\n",
    "        return self.fc(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cce83a",
   "metadata": {},
   "source": [
    "## 4) Data Pipeline: CIFAR‑100 with per‑model transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbb521cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_transforms_32(train: bool = True, use_randaugment: bool = True, random_erasing: bool = True):\n",
    "    aug = []\n",
    "    if train:\n",
    "        aug += [transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip()]\n",
    "        if use_randaugment and hasattr(transforms, \"RandAugment\"):\n",
    "            aug += [transforms.RandAugment(num_ops=2, magnitude=9)]\n",
    "    aug += [transforms.ToTensor(), transforms.Normalize(CIFAR_MEAN, CIFAR_STD)]\n",
    "    if train and random_erasing:\n",
    "        aug += [transforms.RandomErasing(p=0.25, scale=(0.02, 0.2))]\n",
    "    return transforms.Compose(aug)\n",
    "\n",
    "def build_transforms_224(img_size: int = 224, train: bool = True, use_randaugment: bool = True):\n",
    "    if train:\n",
    "        ops = [transforms.RandomResizedCrop(img_size, scale=(0.5, 1.0), ratio=(3/4, 4/3)),\n",
    "               transforms.RandomHorizontalFlip()]\n",
    "        if use_randaugment and hasattr(transforms, \"RandAugment\"):\n",
    "            ops += [transforms.RandAugment(num_ops=2, magnitude=9)]\n",
    "    else:\n",
    "        ops = [transforms.Resize(img_size), transforms.CenterCrop(img_size)]\n",
    "    ops += [transforms.ToTensor(), transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)]\n",
    "    return transforms.Compose(ops)\n",
    "\n",
    "def get_dataloaders_for_model(model_name: str, img_size: int, batch_size: int,\n",
    "                              data_root: str, val_split: float = 0.1, num_workers: int = 0, seed: int = 42):\n",
    "    small_32 = {\"wrn28x10\", \"wide_resnet_28x10\"}\n",
    "    if model_name in small_32:\n",
    "        train_tf = build_transforms_32(train=True,  use_randaugment=True, random_erasing=True)\n",
    "        test_tf  = build_transforms_32(train=False, use_randaugment=False, random_erasing=False)\n",
    "    else:\n",
    "        train_tf = build_transforms_224(img_size, train=True,  use_randaugment=True)\n",
    "        test_tf  = build_transforms_224(img_size, train=False, use_randaugment=False)\n",
    "\n",
    "    full_train = datasets.CIFAR100(root=data_root, train=True, download=True, transform=train_tf)\n",
    "    test_set   = datasets.CIFAR100(root=data_root, train=False, download=True, transform=test_tf)\n",
    "    val_len = int(len(full_train) * val_split); train_len = len(full_train) - val_len\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    train_set, val_set = random_split(full_train, [train_len, val_len], generator=g)\n",
    "\n",
    "    persistent = False\n",
    "    dl_train = DataLoader(train_set, batch_size=batch_size, shuffle=True,\n",
    "                          num_workers=num_workers, pin_memory=True, drop_last=True,\n",
    "                          persistent_workers=persistent)\n",
    "    dl_val   = DataLoader(val_set, batch_size=batch_size, shuffle=False,\n",
    "                          num_workers=num_workers, pin_memory=True,\n",
    "                          persistent_workers=persistent)\n",
    "    dl_test  = DataLoader(test_set, batch_size=batch_size, shuffle=False,\n",
    "                          num_workers=num_workers, pin_memory=True,\n",
    "                          persistent_workers=persistent)\n",
    "    class_names = getattr(test_set, 'classes', [str(i) for i in range(100)])\n",
    "    return dl_train, dl_val, dl_test, class_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae81ff20",
   "metadata": {},
   "source": [
    "## 5) Model Factory (torchvision + timm, plus ViT‑Hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7db93bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model(name: str, num_classes: int, pretrained: bool = False) -> Tuple[nn.Module, int]:\n",
    "    name = name.lower()\n",
    "    if name in [\"resnet18\", \"resnet34\", \"resnet50\"]:\n",
    "        import torchvision.models as tvm\n",
    "        if name == \"resnet18\":\n",
    "            model = tvm.resnet18(weights=tvm.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "        elif name == \"resnet34\":\n",
    "            model = tvm.resnet34(weights=tvm.ResNet34_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "        else:\n",
    "            model = tvm.resnet50(weights=tvm.ResNet50_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "        return model, 224\n",
    "\n",
    "    if name in [\"wrn28x10\", \"wide_resnet_28x10\"]:\n",
    "        model = WideResNet(depth=28, widen_factor=10, num_classes=num_classes, drop_rate=0.3)\n",
    "        return model, 32\n",
    "\n",
    "    if name in [\"convnext_tiny\", \"convnext_t\"]:\n",
    "        model = timm.create_model(\"convnext_tiny\", pretrained=pretrained, num_classes=num_classes)\n",
    "        return model, 224\n",
    "\n",
    "    if name in [\"vit_tiny\", \"vit_tiny_patch16_224\", \"deit_tiny\"]:\n",
    "        model_name = \"deit_tiny_patch16_224\" if name in [\"deit_tiny\"] else \"vit_tiny_patch16_224\"\n",
    "        model = timm.create_model(model_name, pretrained=pretrained, num_classes=num_classes)\n",
    "        return model, 224\n",
    "\n",
    "    if name in [\"vit_hybrid\", \"vit_small_r26_s32_224\"]:\n",
    "        model = timm.create_model(\"vit_small_r26_s32_224\", pretrained=pretrained, num_classes=num_classes)\n",
    "        return model, 224\n",
    "\n",
    "    raise ValueError(f\"Unknown model name: {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f7f564e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Patch] create_model() wrapped to inject head Dropout (default p=0.2; override via MODEL_CFG['head_dropout']).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Patch: wrap original create_model to inject classifier head Dropout across backbones ===\n",
    "try:\n",
    "    _original_create_model\n",
    "except NameError:\n",
    "    _original_create_model = create_model  # keep reference to the original\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def _wrap_linear_with_dropout(module, p, num_classes):\n",
    "    if isinstance(module, nn.Sequential):\n",
    "        # if a Dropout already present, keep as is\n",
    "        if any(isinstance(m, nn.Dropout) for m in module.modules()):\n",
    "            return module\n",
    "        # if ends with Linear, wrap last Linear\n",
    "        if isinstance(module[-1], nn.Linear):\n",
    "            in_feats = module[-1].in_features\n",
    "            return nn.Sequential(*list(module[:-1]), nn.Dropout(p), nn.Linear(in_feats, num_classes))\n",
    "        return nn.Sequential(nn.Dropout(p), *list(module))\n",
    "\n",
    "    if isinstance(module, nn.Linear):\n",
    "        in_feats = module.in_features\n",
    "        return nn.Sequential(nn.Dropout(p), nn.Linear(in_feats, num_classes))\n",
    "\n",
    "    return module\n",
    "\n",
    "def create_model(name: str, num_classes: int, pretrained: bool = False):\n",
    "    model, img_size = _original_create_model(name, num_classes, pretrained)\n",
    "    key = name.lower()\n",
    "    cfg_local = MODEL_CFG.get(key, {}) if 'MODEL_CFG' in globals() else {}\n",
    "    head_p = float(cfg_local.get('head_dropout', 0.2))\n",
    "\n",
    "    # Torchvision ResNet family: model.fc\n",
    "    if hasattr(model, \"fc\"):\n",
    "        try:\n",
    "            model.fc = _wrap_linear_with_dropout(model.fc, head_p, num_classes)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # ConvNeXt variants: classifier\n",
    "    if hasattr(model, \"classifier\"):\n",
    "        try:\n",
    "            model.classifier = _wrap_linear_with_dropout(model.classifier, head_p, num_classes)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # timm/torchvision heads\n",
    "    if hasattr(model, \"head\"):\n",
    "        try:\n",
    "            model.head = _wrap_linear_with_dropout(model.head, head_p, num_classes)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # torchvision ViT: 'heads' (Sequential([...], Linear))\n",
    "    if hasattr(model, \"heads\"):\n",
    "        try:\n",
    "            if isinstance(model.heads, nn.Sequential) and isinstance(model.heads[-1], nn.Linear):\n",
    "                in_feats = model.heads[-1].in_features\n",
    "                has_dropout = any(isinstance(m, nn.Dropout) for m in model.heads.modules())\n",
    "                if not has_dropout:\n",
    "                    new_seq = []\n",
    "                    for m in model.heads[:-1]:\n",
    "                        new_seq.append(m)\n",
    "                    new_seq.append(nn.Dropout(head_p))\n",
    "                    new_seq.append(nn.Linear(in_feats, num_classes))\n",
    "                    model.heads = nn.Sequential(*new_seq)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return model, img_size\n",
    "\n",
    "print(\"[Patch] create_model() wrapped to inject head Dropout (default p=0.2; override via MODEL_CFG['head_dropout']).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73341833",
   "metadata": {},
   "source": [
    "## 6) SOTA Add‑ons: Mixup/CutMix + EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "082c8999",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rand_bbox(W: int, H: int, lam: float):\n",
    "    cut_rat = math.sqrt(1. - lam)\n",
    "    cut_w, cut_h = int(W * cut_rat), int(H * cut_rat)\n",
    "    cx, cy = np.random.randint(W), np.random.randint(H)\n",
    "    x1 = np.clip(cx - cut_w // 2, 0, W); y1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    x2 = np.clip(cx + cut_w // 2, 0, W); y2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def apply_mixup_cutmix(images: torch.Tensor, targets: torch.Tensor, num_classes: int,\n",
    "                       mixup_alpha: float = 0.2, cutmix_alpha: float = 1.0,\n",
    "                       prob: float = 1.0, switch_prob: float = 0.5) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    if np.random.rand() > prob:\n",
    "        return images, one_hot(targets, num_classes, smoothing=0.0)\n",
    "\n",
    "    use_cutmix = np.random.rand() < switch_prob\n",
    "    lam = np.random.beta(cutmix_alpha, cutmix_alpha) if use_cutmix else np.random.beta(mixup_alpha, mixup_alpha)\n",
    "    batch_size = images.size(0)\n",
    "    index = torch.randperm(batch_size, device=images.device)\n",
    "\n",
    "    if use_cutmix:\n",
    "        _, H, W = images.size(1), images.size(2), images.size(3)\n",
    "        x1, y1, x2, y2 = rand_bbox(W, H, lam)\n",
    "        images[:, :, y1:y2, x1:x2] = images[index, :, y1:y2, x1:x2]\n",
    "        lam = 1 - ((x2 - x1) * (y2 - y1) / (W * H + 1e-6))\n",
    "\n",
    "    mixed_targets = lam * one_hot(targets, num_classes) + (1 - lam) * one_hot(targets[index], num_classes)\n",
    "    if not use_cutmix:\n",
    "        images = lam * images + (1 - lam) * images[index]\n",
    "    return images, mixed_targets\n",
    "\n",
    "class ModelEMA:\n",
    "    def __init__(self, model: nn.Module, decay: float = 0.9999, device: Optional[torch.device] = None):\n",
    "        self.decay = decay\n",
    "        self.device = device\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone().detach()\n",
    "                if device is not None:\n",
    "                    self.shadow[name] = self.shadow[name].to(device)\n",
    "\n",
    "    def update(self, model: nn.Module):\n",
    "        with torch.no_grad():\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    self.shadow[name].mul_(self.decay).add_(param.data, alpha=1.0 - self.decay)\n",
    "\n",
    "    def apply_shadow(self, model: nn.Module):\n",
    "        self.backup = {}\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                param.data.copy_(self.shadow[name].to(param.data.device))\n",
    "\n",
    "    def restore(self, model: nn.Module):\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and name in self.backup:\n",
    "                param.data.copy_(self.backup[name])\n",
    "        self.backup = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4525b100",
   "metadata": {},
   "source": [
    "## 7) Train/Eval: AMP + channels‑last + accumulation + cosine LR + EMA/Mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3eeb020",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model, dl, optimizer, scheduler, scaler, device, criterion,\n",
    "                    max_grad_norm=None, accum_steps: int = 1, use_channels_last: bool = True,\n",
    "                    use_mix: bool = False, num_classes: int = 100, mix_params: Dict = None, ema_obj: Optional[ModelEMA] = None):\n",
    "    model.train()\n",
    "    running_loss = 0.0; correct1 = 0; total = 0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    mix_params = mix_params or {}\n",
    "    for step, (images, targets) in enumerate(dl, start=1):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        if use_channels_last and torch.cuda.is_available():\n",
    "            images = images.to(memory_format=torch.channels_last)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        soft_targets = None\n",
    "        if use_mix:\n",
    "            images, soft_targets = apply_mixup_cutmix(\n",
    "                images, targets, num_classes,\n",
    "                mixup_alpha=mix_params.get('mixup_alpha', 0.2),\n",
    "                cutmix_alpha=mix_params.get('cutmix_alpha', 1.0),\n",
    "                prob=mix_params.get('prob', 1.0),\n",
    "                switch_prob=mix_params.get('switch_prob', 0.5)\n",
    "            )\n",
    "\n",
    "        with torch.amp.autocast('cuda', enabled=torch.cuda.is_available()):\n",
    "            logits = model(images)\n",
    "            if soft_targets is not None:\n",
    "                loss = soft_cross_entropy(logits, soft_targets) / accum_steps\n",
    "            else:\n",
    "                loss = criterion(logits, targets) / accum_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if step % accum_steps == 0:\n",
    "            if max_grad_norm is not None:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            if ema_obj is not None:\n",
    "                ema_obj.update(model)\n",
    "\n",
    "        running_loss += loss.item() * images.size(0) * accum_steps\n",
    "        acc1, = topk_accuracy(logits, targets, ks=(1,))\n",
    "        correct1 += int(acc1 * images.size(0))\n",
    "        total += images.size(0)\n",
    "    return running_loss / total, correct1 / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dl, device, criterion, use_channels_last: bool = True,\n",
    "             ema_obj: Optional[ModelEMA] = None, use_ema_eval: bool = False):\n",
    "    # Use EMA weights if requested\n",
    "    if ema_obj is not None and use_ema_eval:\n",
    "        ema_obj.apply_shadow(model)\n",
    "\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    correct1 = 0\n",
    "    correct5 = 0\n",
    "    all_probs, all_preds, all_targets, all_logits = [], [], [], []\n",
    "\n",
    "    ce = torch.nn.CrossEntropyLoss()  # plain CE for evaluation\n",
    "\n",
    "    for images, targets in dl:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        if use_channels_last and torch.cuda.is_available():\n",
    "            images = images.to(memory_format=torch.channels_last)\n",
    "        targets = targets.to(device, non_blocking=True).long()\n",
    "\n",
    "        with torch.amp.autocast('cuda', enabled=False):\n",
    "            logits = model(images).float()\n",
    "            loss = ce(logits, targets)\n",
    "\n",
    "        running_loss += float(loss.item()) * images.size(0)\n",
    "\n",
    "        acc1, acc5 = topk_accuracy(logits, targets, ks=(1, 5))\n",
    "        correct1 += int(acc1 * images.size(0))\n",
    "        correct5 += int(acc5 * images.size(0))\n",
    "\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        conf, pred = probs.max(dim=1)\n",
    "        all_probs.append(conf.detach().cpu().numpy())\n",
    "        all_preds.append(pred.detach().cpu().numpy())\n",
    "        all_targets.append(targets.detach().cpu().numpy())\n",
    "        all_logits.append(logits.detach().cpu())\n",
    "\n",
    "        total += images.size(0)\n",
    "\n",
    "    if ema_obj is not None and use_ema_eval:\n",
    "        ema_obj.restore(model)\n",
    "\n",
    "    avg_loss = running_loss / max(1, total)\n",
    "    top1 = correct1 / max(1, total)\n",
    "    top5 = correct5 / max(1, total)\n",
    "\n",
    "    all_probs = np.concatenate(all_probs, axis=0) if len(all_probs) else np.array([])\n",
    "    all_preds = np.concatenate(all_preds, axis=0) if len(all_preds) else np.array([])\n",
    "    all_targets = np.concatenate(all_targets, axis=0) if len(all_targets) else np.array([])\n",
    "    all_logits = torch.cat(all_logits, 0) if len(all_logits) else torch.empty(0)\n",
    "\n",
    "    correctness = (all_preds == all_targets).astype(np.float32) if len(all_preds) else np.array([])\n",
    "    return avg_loss, top1, top5, all_probs, all_preds, all_targets, correctness, all_logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af39b239",
   "metadata": {},
   "source": [
    "## 8) Plotting Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "711d7296",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_training_curves(history: Dict[str, List[float]], out_path: str, title: str):\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(history[\"train_acc\"], label=\"train_acc\")\n",
    "    plt.plot(history[\"val_acc\"], label=\"val_acc\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.title(f\"{title} - Accuracy\")\n",
    "    plt.legend(); plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout(); plt.savefig(os.path.join(out_path, \"training_accuracy.png\"), dpi=150); plt.close()\n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(history[\"train_loss\"], label=\"train_loss\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(f\"{title} - Loss\")\n",
    "    plt.legend(); plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout(); plt.savefig(os.path.join(out_path, \"training_loss.png\"), dpi=150); plt.close()\n",
    "\n",
    "def plot_confusion_matrix(cm: np.ndarray, out_path: str, title: str):\n",
    "    plt.figure(figsize=(9,8))\n",
    "    im = plt.imshow(cm, interpolation='nearest', aspect='auto')\n",
    "    plt.title(title); plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    plt.tight_layout(); plt.savefig(os.path.join(out_path, \"confusion_matrix.png\"), dpi=160); plt.close()\n",
    "\n",
    "def plot_per_class_accuracy(cm: np.ndarray, out_path: str, title: str, class_names: List[str], topn: int = 25):\n",
    "    per_class = cm.diagonal() / cm.sum(axis=1).clip(min=1)\n",
    "    idx_sorted = np.argsort(per_class); worst_idx = idx_sorted[:topn]\n",
    "    labels = [class_names[i] if i < len(class_names) else str(i) for i in worst_idx]\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.bar(np.arange(topn), per_class[worst_idx])\n",
    "    plt.ylim(0, 1.0); plt.xticks(np.arange(topn), labels, rotation=90)\n",
    "    plt.ylabel(\"Per-class accuracy\"); plt.title(f\"{title} - {topn} Lowest Classes\")\n",
    "    plt.tight_layout(); plt.savefig(os.path.join(out_path, \"per_class_accuracy_worst.png\"), dpi=150); plt.close()\n",
    "\n",
    "def plot_reliability(bin_stats: Dict, out_path: str, title: str, suffix: str):\n",
    "    left = np.array(bin_stats[\"bin_left\"]); right = np.array(bin_stats[\"bin_right\"])\n",
    "    acc = np.array(bin_stats[\"bin_acc\"]); centers = (left + right) / 2.0\n",
    "    width = (right - left) * 0.9\n",
    "    plt.figure(figsize=(6.5,6))\n",
    "    plt.bar(centers, acc, align='center', width=width, label=\"Accuracy per bin\")\n",
    "    plt.plot([0,1],[0,1], linestyle='--', label=\"Perfect calibration\")\n",
    "    plt.xlabel(\"Confidence\"); plt.ylabel(\"Accuracy\"); plt.title(f\"{title} - Reliability Diagram ({suffix})\")\n",
    "    plt.legend(); plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout(); plt.savefig(os.path.join(out_path, f\"reliability_diagram_{suffix}.png\"), dpi=160); plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba4db87",
   "metadata": {},
   "source": [
    "## 9) Configuration & Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f0d9010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATA_ROOT = './data'\n",
    "VAL_SPLIT = 0.1\n",
    "NUM_WORKERS = 2 if os.name == 'nt' else 4\n",
    "SEED = 42\n",
    "DETERMINISTIC = False\n",
    "LABEL_SMOOTHING = 0.1\n",
    "USE_CHANNELS_LAST = True\n",
    "REQUIRE_GPU = True\n",
    "\n",
    "LOG_TENSORBOARD = True\n",
    "LOG_WANDB = False\n",
    "WANDB_PROJECT = \"cifar100-capstone\"\n",
    "WANDB_ENTITY = None\n",
    "\n",
    "EARLY_STOP_DEFAULT = {'enabled': True, 'patience': 12, 'min_delta': 5e-4, 'min_epochs': 15, 'target_top1': None}\n",
    "\n",
    "MODEL_CFG = {\n",
    "    'resnet18': {\n",
    "        'pretrained': True, 'epochs': 100, 'batch': 32, 'accum': 2,\n",
    "        'opt': 'sgd', 'lr': 0.05, 'wd': 1e-4, 'max_grad_norm': None,\n",
    "        'mix': {'enabled': True, 'prob': 0.8, 'mixup_alpha': 0.2, 'cutmix_alpha': 1.0, 'switch_prob': 0.5},\n",
    "        'ema': {'enabled': True, 'decay': 0.9999, 'eval': True},\n",
    "        'early_stop': {'patience': 15, 'min_epochs': 20, 'target_top1': 0.72}\n",
    "    },\n",
    "    'wrn28x10': {\n",
    "        'pretrained': False, 'epochs': 120, 'batch': 128, 'accum': 1,\n",
    "        'opt': 'sgd', 'lr': 0.1, 'wd': 5e-4, 'max_grad_norm': None, 'drop_rate': 0.3,\n",
    "        'mix': {'enabled': True, 'prob': 0.8, 'mixup_alpha': 0.2, 'cutmix_alpha': 1.0, 'switch_prob': 0.5},\n",
    "        'ema': {'enabled': True, 'decay': 0.9999, 'eval': True},\n",
    "        'early_stop': {'patience': 18, 'min_epochs': 25, 'target_top1': 0.78}\n",
    "    },\n",
    "    'convnext_tiny': {\n",
    "        'pretrained': True, 'epochs': 80, 'batch': 32, 'accum': 2,\n",
    "        'opt': 'adamw', 'lr': 2e-4, 'wd': 0.05, 'max_grad_norm': 1.0,\n",
    "        'mix': {'enabled': True, 'prob': 0.8, 'mixup_alpha': 0.2, 'cutmix_alpha': 1.0, 'switch_prob': 0.5},\n",
    "        'ema': {'enabled': True, 'decay': 0.9999, 'eval': True},\n",
    "        'early_stop': {'patience': 12, 'min_epochs': 15, 'target_top1': 0.78}\n",
    "    },\n",
    "    'vit_tiny': {\n",
    "        'pretrained': True, 'epochs': 100, 'batch': 32, 'accum': 2,\n",
    "        'opt': 'adamw', 'lr': 5e-4, 'wd': 0.05, 'max_grad_norm': 1.0,\n",
    "        'mix': {'enabled': True, 'prob': 0.8, 'mixup_alpha': 0.2, 'cutmix_alpha': 1.0, 'switch_prob': 0.5},\n",
    "        'ema': {'enabled': True, 'decay': 0.9999, 'eval': True},\n",
    "        'early_stop': {'patience': 15, 'min_epochs': 20, 'target_top1': 0.75}\n",
    "    },\n",
    "    'vit_hybrid': {\n",
    "        'pretrained': True, 'epochs': 100, 'batch': 32, 'accum': 2,\n",
    "        'opt': 'adamw', 'lr': 5e-4, 'wd': 0.05, 'max_grad_norm': 1.0,\n",
    "        'mix': {'enabled': True, 'prob': 0.8, 'mixup_alpha': 0.2, 'cutmix_alpha': 1.0, 'switch_prob': 0.5},\n",
    "        'ema': {'enabled': True, 'decay': 0.9999, 'eval': True},\n",
    "        'early_stop': {'patience': 15, 'min_epochs': 20, 'target_top1': 0.78}\n",
    "    },\n",
    "}\n",
    "\n",
    "OUTPUT_ROOT = \"./outputs\"\n",
    "\n",
    "if REQUIRE_GPU:\n",
    "    assert torch.cuda.is_available(), \"No GPU detected. Set REQUIRE_GPU=False to allow CPU (very slow).\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "seed_everything(SEED, deterministic=DETERMINISTIC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c8e4cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Overrides] Applied config updates for resnet18, wrn28x10, convnext_tiny, vit_tiny, vit_hybrid.\n",
      "[Overrides] Set EMA.eval=False for validation across all models (you can re-enable later).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === CONFIG OVERRIDES (added) ===\n",
    "# Apply recommended updates without editing the original MODEL_CFG literal.\n",
    "\n",
    "# Safer early-stop for all models: remove hard target gate.\n",
    "for _name in ['resnet18', 'wrn28x10', 'convnext_tiny', 'vit_tiny', 'vit_hybrid']:\n",
    "    if _name in MODEL_CFG:\n",
    "        es = MODEL_CFG[_name].get('early_stop', {})\n",
    "        es.update({'target_top1': None})\n",
    "        MODEL_CFG[_name]['early_stop'] = es\n",
    "\n",
    "# ResNet-18 tweaks\n",
    "if 'resnet18' in MODEL_CFG:\n",
    "    MODEL_CFG['resnet18'].update({\n",
    "        'wd': 5e-4,\n",
    "        'head_dropout': 0.1,\n",
    "        'mix': {'enabled': True, 'prob': 0.6, 'mixup_alpha': 0.2, 'cutmix_alpha': 1.0, 'switch_prob': 0.5},\n",
    "    })\n",
    "    es = MODEL_CFG['resnet18'].get('early_stop', {})\n",
    "    es.update({'patience': 15, 'min_epochs': 60})\n",
    "    MODEL_CFG['resnet18']['early_stop'] = es\n",
    "\n",
    "# WRN-28x10 tweaks\n",
    "if 'wrn28x10' in MODEL_CFG:\n",
    "    MODEL_CFG['wrn28x10'].update({\n",
    "        'epochs': 160,\n",
    "        'mix': {'enabled': True, 'prob': 0.5, 'mixup_alpha': 0.2, 'cutmix_alpha': 1.0, 'switch_prob': 0.5},\n",
    "    })\n",
    "    es = MODEL_CFG['wrn28x10'].get('early_stop', {})\n",
    "    es.update({'patience': 20, 'min_epochs': 120})\n",
    "    MODEL_CFG['wrn28x10']['early_stop'] = es\n",
    "\n",
    "# ConvNeXt / ViT early-stop smoothing\n",
    "for _name, _me in [('convnext_tiny', 50), ('vit_tiny', 60), ('vit_hybrid', 60)]:\n",
    "    if _name in MODEL_CFG:\n",
    "        es = MODEL_CFG[_name].get('early_stop', {})\n",
    "        es.update({'patience': 15, 'min_epochs': _me})\n",
    "        MODEL_CFG[_name]['early_stop'] = es\n",
    "\n",
    "print(\"[Overrides] Applied config updates for resnet18, wrn28x10, convnext_tiny, vit_tiny, vit_hybrid.\")\n",
    "\n",
    "\n",
    "# === CONFIG OVERRIDES (final) ===\n",
    "# Extend prior overrides: explicitly turn off EMA for evaluation to simplify early validation behavior.\n",
    "for _name, _cfg in MODEL_CFG.items():\n",
    "    if 'ema' in _cfg:\n",
    "        _ema = _cfg['ema']\n",
    "        _ema['eval'] = False\n",
    "        _cfg['ema'] = _ema\n",
    "print(\"[Overrides] Set EMA.eval=False for validation across all models (you can re-enable later).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd130e2c",
   "metadata": {},
   "source": [
    "## 10) Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbcfbafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate_one_model(model_name: str):\n",
    "    cfg = MODEL_CFG[model_name]\n",
    "    pretrained = cfg.get('pretrained', False)\n",
    "    model, IMG_SIZE = create_model(model_name, num_classes=100, pretrained=pretrained)\n",
    "    if isinstance(model, WideResNet) and ('drop_rate' in cfg):\n",
    "        model = WideResNet(depth=28, widen_factor=10, num_classes=100, drop_rate=cfg['drop_rate'])\n",
    "    model = model.to(DEVICE)\n",
    "    if USE_CHANNELS_LAST and torch.cuda.is_available():\n",
    "        model = model.to(memory_format=torch.channels_last)\n",
    "    if hasattr(model, \"set_grad_checkpointing\"):\n",
    "        model.set_grad_checkpointing(True)\n",
    "\n",
    "    dl_train, dl_val, dl_test, class_names = get_dataloaders_for_model(\n",
    "        model_name, IMG_SIZE, cfg['batch'], data_root=DATA_ROOT, val_split=VAL_SPLIT,\n",
    "        num_workers=NUM_WORKERS, seed=SEED\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=(0.0 if cfg.get('mix',{}).get('enabled', False) else LABEL_SMOOTHING)).to(DEVICE)\n",
    "\n",
    "    opt_name = cfg['opt']; lr = cfg['lr']; wd = cfg['wd']\n",
    "    if opt_name == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd, nesterov=True)\n",
    "    elif opt_name == \"adamw\":\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "    EPOCHS = cfg['epochs']; ACCUM_STEPS = max(1, int(cfg['accum']))\n",
    "    warmup_epochs = max(1, int(0.03 * EPOCHS))\n",
    "    total_steps = math.ceil(EPOCHS * len(dl_train) / ACCUM_STEPS)\n",
    "    warmup_steps = math.ceil(warmup_epochs * len(dl_train) / ACCUM_STEPS)\n",
    "    def cosine_lr_lambda(step):\n",
    "        if step < warmup_steps: return float(step) / float(max(1, warmup_steps))\n",
    "        progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=cosine_lr_lambda)\n",
    "\n",
    "    early = {**EARLY_STOP_DEFAULT, **cfg.get('early_stop', {})}\n",
    "    patience = int(early.get('patience', 10))\n",
    "    min_delta = float(early.get('min_delta', 0.0))\n",
    "    min_epochs = int(early.get('min_epochs', 0))\n",
    "    target_top1 = early.get('target_top1', None)\n",
    "    early_enabled = bool(early.get('enabled', True))\n",
    "\n",
    "    ema = None\n",
    "    ema_cfg = cfg.get('ema', {'enabled': False})\n",
    "    if ema_cfg.get('enabled', False):\n",
    "        ema = ModelEMA(model, decay=ema_cfg.get('decay', 0.9999), device=None)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    run_name = f\"{model_name}_e{EPOCHS}_bs{cfg['batch']}_acc{ACCUM_STEPS}_{timestamp}\"\n",
    "    OUT_DIR = os.path.join(OUTPUT_ROOT, run_name)\n",
    "    CKPT_DIR = os.path.join(OUT_DIR, \"checkpoints\")\n",
    "    PLOTS_DIR = os.path.join(OUT_DIR, \"plots\")\n",
    "    RESULTS_DIR = os.path.join(OUT_DIR, \"results\")\n",
    "    for d in [OUT_DIR, CKPT_DIR, PLOTS_DIR, RESULTS_DIR]: ensure_dir(d)\n",
    "\n",
    "    tb_writer = None\n",
    "    if LOG_TENSORBOARD:\n",
    "        try:\n",
    "            from torch.utils.tensorboard import SummaryWriter\n",
    "            tb_logdir = os.path.join(OUT_DIR, \"tb\"); ensure_dir(tb_logdir)\n",
    "            tb_writer = SummaryWriter(log_dir=tb_logdir)\n",
    "        except Exception as e:\n",
    "            print(\"TensorBoard unavailable:\", e); tb_writer = None\n",
    "\n",
    "    wb = None\n",
    "    if LOG_WANDB:\n",
    "        try:\n",
    "            import wandb\n",
    "            wb = wandb.init(project=WANDB_PROJECT, entity=WANDB_ENTITY, name=run_name,\n",
    "                            config={\n",
    "                                \"model\": model_name, \"epochs\": EPOCHS, \"batch\": cfg['batch'], \"accum\": ACCUM_STEPS,\n",
    "                                \"optimizer\": opt_name, \"lr\": lr, \"weight_decay\": wd,\n",
    "                                \"img_size\": IMG_SIZE, \"mix\": cfg.get('mix', {}), \"ema\": ema_cfg,\n",
    "                                \"early_stop\": early,\n",
    "                            })\n",
    "        except Exception as e:\n",
    "            print(\"W&B unavailable or not logged in:\", e); wb = None\n",
    "\n",
    "    hist_csv_path = os.path.join(RESULTS_DIR, \"history.csv\")\n",
    "    with open(hist_csv_path, \"w\") as f:\n",
    "        f.write(\"epoch,train_loss,train_acc,val_loss,val_acc,lr\\n\")\n",
    "\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=torch.cuda.is_available())\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "    best_val = -1.0\n",
    "    epochs_no_improve = 0\n",
    "    BEST_PATH = os.path.join(CKPT_DIR, f\"{model_name}_best.pt\")\n",
    "\n",
    "    print(f\"\\n==> [{model_name}] Training for {EPOCHS} epochs | opt={opt_name} lr={lr} wd={wd} | \"\n",
    "          f\"img={IMG_SIZE} | bs={cfg['batch']} | accum={ACCUM_STEPS}\")\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        t0 = time.time()\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, dl_train, optimizer, scheduler, scaler, DEVICE, criterion,\n",
    "            max_grad_norm=cfg.get('max_grad_norm', None), accum_steps=ACCUM_STEPS,\n",
    "            use_channels_last=USE_CHANNELS_LAST,\n",
    "            use_mix=cfg.get('mix',{}).get('enabled', False),\n",
    "            num_classes=100, mix_params=cfg.get('mix', {}), ema_obj=ema\n",
    "        )\n",
    "\n",
    "        val_loss, val_top1, _, _, _, _, _, _ = evaluate(\n",
    "            model, dl_val, DEVICE, criterion, use_channels_last=USE_CHANNELS_LAST,\n",
    "            ema_obj=ema, use_ema_eval=ema_cfg.get('eval', True)\n",
    "        )\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss); history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_acc\"].append(train_acc);   history[\"val_acc\"].append(val_top1)\n",
    "\n",
    "        lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "        with open(hist_csv_path, \"a\") as f:\n",
    "            f.write(f\"{epoch},{train_loss:.6f},{train_acc:.6f},{val_loss:.6f},{val_top1:.6f},{lr_now:.8f}\\n\")\n",
    "\n",
    "        if tb_writer is not None:\n",
    "            tb_writer.add_scalar(\"train/loss\", train_loss, epoch)\n",
    "            tb_writer.add_scalar(\"train/acc1\", train_acc, epoch)\n",
    "            tb_writer.add_scalar(\"val/loss\", val_loss, epoch)\n",
    "            tb_writer.add_scalar(\"val/acc1\", val_top1, epoch)\n",
    "            tb_writer.add_scalar(\"opt/lr\", lr_now, epoch)\n",
    "\n",
    "        if wb is not None:\n",
    "            wb.log({\"train/loss\": train_loss, \"train/acc1\": train_acc, \"val/loss\": val_loss, \"val/acc1\": val_top1, \"opt/lr\": lr_now}, step=epoch)\n",
    "\n",
    "        print(f\"  Epoch {epoch:03d}/{EPOCHS} | train_loss={train_loss:.4f} acc={train_acc*100:.2f}% | \"\n",
    "              f\"val_loss={val_loss:.4f} acc@1={val_top1*100:.2f}% | time={time.time()-t0:.1f}s\")\n",
    "\n",
    "        improved = val_top1 > (best_val + min_delta)\n",
    "        if improved:\n",
    "            best_val = val_top1\n",
    "            epochs_no_improve = 0\n",
    "            torch.save({'model': model.state_dict(), 'epoch': epoch, 'val_acc1': best_val}, BEST_PATH)\n",
    "            if wb is not None:\n",
    "                wb.log({\"checkpoint/best_epoch\": epoch, \"checkpoint/best_val_acc1\": best_val}, step=epoch)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if early_enabled and (target_top1 is not None) and (val_top1 >= float(target_top1)):\n",
    "            print(f\"  Early stop: target_top1 {float(target_top1):.3f} reached at epoch {epoch}.\")\n",
    "            break\n",
    "        if early_enabled and (epoch >= min_epochs) and (epochs_no_improve >= patience):\n",
    "            print(f\"  Early stop: no improvement > {min_delta} for {patience} epochs (epoch {epoch}).\")\n",
    "            break\n",
    "\n",
    "    try:\n",
    "        ckpt = torch.load(BEST_PATH, map_location='cpu', weights_only=True)\n",
    "    except TypeError:\n",
    "        ckpt = torch.load(BEST_PATH, map_location='cpu')\n",
    "    model.load_state_dict(ckpt['model'])\n",
    "\n",
    "    use_ema_eval = ema_cfg.get('eval', True)\n",
    "    val_loss, val_top1, val_top5, val_conf, val_pred, val_true, val_corr, val_logits = evaluate(\n",
    "        model, dl_val, DEVICE, criterion, use_channels_last=USE_CHANNELS_LAST, ema_obj=ema, use_ema_eval=use_ema_eval\n",
    "    )\n",
    "    test_loss, test_top1, test_top5, test_conf, test_pred, test_true, test_corr, test_logits = evaluate(\n",
    "        model, dl_test, DEVICE, criterion, use_channels_last=USE_CHANNELS_LAST, ema_obj=ema, use_ema_eval=use_ema_eval\n",
    "    )\n",
    "\n",
    "    cm = confusion_matrix(test_true, test_pred, labels=list(range(100)))\n",
    "\n",
    "    T = fit_temperature(val_logits, torch.from_numpy(val_true))\n",
    "    ece_raw, bin_stats_raw = compute_ece(test_conf, test_corr, n_bins=15)\n",
    "    test_probs_scaled = F.softmax(test_logits / T, dim=1).numpy()\n",
    "    test_conf_scaled = test_probs_scaled.max(axis=1)\n",
    "    ece_temp, bin_stats_temp = compute_ece(test_conf_scaled, test_corr, n_bins=15)\n",
    "\n",
    "    metrics = {\n",
    "        'model': model_name,\n",
    "        'epochs_run': len(history['val_acc']),\n",
    "        'optimizer': opt_name, 'lr_final': float(optimizer.param_groups[0]['lr']), 'weight_decay': float(optimizer.param_groups[0]['weight_decay']),\n",
    "        'batch_size': cfg['batch'], 'accum_steps': ACCUM_STEPS,\n",
    "        'max_grad_norm': cfg.get('max_grad_norm', None),\n",
    "        'label_smoothing': (0.0 if cfg.get('mix',{}).get('enabled', False) else float(LABEL_SMOOTHING)),\n",
    "        'mixup_cutmix': cfg.get('mix', {}),\n",
    "        'ema': ema_cfg,\n",
    "        'val': {'loss': float(val_loss), 'top1': float(val_top1), 'top5': float(val_top5)},\n",
    "        'test': {'loss': float(test_loss), 'top1': float(test_top1), 'top5': float(test_top5)},\n",
    "        'ece_raw': float(ece_raw), 'ece_temp_scaled': float(ece_temp), 'temperature': float(T),\n",
    "        'checkpoint': os.path.join(OUT_DIR, \"checkpoints\", f\"{model_name}_best.pt\"), 'run_dir': OUT_DIR\n",
    "    }\n",
    "    RESULTS_DIR = os.path.join(OUT_DIR, \"results\")\n",
    "    with open(os.path.join(RESULTS_DIR, 'metrics.json'), 'w') as f: json.dump(metrics, f, indent=2)\n",
    "    with open(os.path.join(RESULTS_DIR, 'reliability_bins_raw.json'), 'w') as f: json.dump(bin_stats_raw, f, indent=2)\n",
    "    with open(os.path.join(RESULTS_DIR, 'reliability_bins_temp.json'), 'w') as f: json.dump(bin_stats_temp, f, indent=2)\n",
    "    np = __import__('numpy')\n",
    "    np.save(os.path.join(RESULTS_DIR, 'confusion_matrix.npy'), cm)\n",
    "\n",
    "    PLOTS_DIR = os.path.join(OUT_DIR, \"plots\")\n",
    "    plot_training_curves(history, PLOTS_DIR, title=model_name.upper())\n",
    "    plot_confusion_matrix(cm, PLOTS_DIR, title=f\"{model_name.upper()} - Confusion Matrix (Test)\")\n",
    "    plot_per_class_accuracy(cm, PLOTS_DIR, title=model_name.upper(), class_names=class_names, topn=25)\n",
    "    plot_reliability(bin_stats_raw,  PLOTS_DIR, title=model_name.upper(), suffix=\"raw\")\n",
    "    plot_reliability(bin_stats_temp, PLOTS_DIR, title=model_name.upper(), suffix=\"temp_scaled\")\n",
    "\n",
    "    if tb_writer is not None:\n",
    "        tb_writer.flush(); tb_writer.close()\n",
    "    try:\n",
    "        import wandb\n",
    "        if wandb.run is not None:\n",
    "            wandb.finish()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(f\"  -> Best checkpoint: {metrics['checkpoint']}\")\n",
    "    print(f\"  -> Plots: {PLOTS_DIR}\")\n",
    "    print(f\"  -> Results: {RESULTS_DIR}\")\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e84bc4",
   "metadata": {},
   "source": [
    "## 11) Train: ResNet‑18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ceded6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data\\cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169M/169M [01:23<00:00, 2.04MB/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-100-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "\n",
      "==> [resnet18] Training for 100 epochs | opt=sgd lr=0.05 wd=0.0005 | img=224 | bs=32 | accum=2\n",
      "  Epoch 001/100 | train_loss=3.1993 acc=22.82% | val_loss=2.0789 acc@1=44.14% | time=364.3s\n",
      "  Epoch 002/100 | train_loss=2.7405 acc=31.05% | val_loss=2.2292 acc@1=43.52% | time=461.8s\n",
      "  Epoch 003/100 | train_loss=2.7263 acc=31.39% | val_loss=2.1251 acc@1=44.18% | time=429.0s\n",
      "  Epoch 004/100 | train_loss=2.5452 acc=34.42% | val_loss=1.9296 acc@1=48.96% | time=371.3s\n",
      "  Epoch 005/100 | train_loss=2.3653 acc=37.37% | val_loss=1.7578 acc@1=53.24% | time=365.1s\n",
      "  Epoch 006/100 | train_loss=2.3231 acc=38.98% | val_loss=1.7403 acc@1=54.12% | time=396.2s\n",
      "  Epoch 007/100 | train_loss=2.3009 acc=38.86% | val_loss=1.8009 acc@1=52.20% | time=376.1s\n",
      "  Epoch 008/100 | train_loss=2.2958 acc=40.77% | val_loss=1.7135 acc@1=54.56% | time=415.2s\n",
      "  Epoch 009/100 | train_loss=2.2813 acc=40.39% | val_loss=1.7543 acc@1=53.74% | time=443.9s\n",
      "  Epoch 010/100 | train_loss=2.2604 acc=41.45% | val_loss=1.7407 acc@1=53.00% | time=454.3s\n",
      "  Epoch 011/100 | train_loss=2.2019 acc=42.04% | val_loss=1.6677 acc@1=55.62% | time=392.0s\n",
      "  Epoch 012/100 | train_loss=2.2187 acc=41.46% | val_loss=1.6759 acc@1=56.04% | time=367.6s\n",
      "  Epoch 013/100 | train_loss=2.1872 acc=41.85% | val_loss=1.6738 acc@1=55.46% | time=395.6s\n",
      "  Epoch 014/100 | train_loss=2.2026 acc=40.89% | val_loss=1.6625 acc@1=55.54% | time=390.5s\n",
      "  Epoch 015/100 | train_loss=2.1868 acc=42.83% | val_loss=1.7078 acc@1=54.04% | time=424.3s\n",
      "  Epoch 016/100 | train_loss=2.1417 acc=43.30% | val_loss=1.6738 acc@1=56.26% | time=350.4s\n",
      "  Epoch 017/100 | train_loss=2.1466 acc=41.69% | val_loss=1.7501 acc@1=53.70% | time=347.5s\n",
      "  Epoch 018/100 | train_loss=2.1719 acc=43.56% | val_loss=1.6186 acc@1=56.14% | time=340.4s\n",
      "  Epoch 019/100 | train_loss=2.1628 acc=42.64% | val_loss=1.5853 acc@1=57.06% | time=347.2s\n",
      "  Epoch 020/100 | train_loss=2.1381 acc=43.38% | val_loss=1.6376 acc@1=57.06% | time=341.5s\n",
      "  Epoch 021/100 | train_loss=2.1146 acc=43.66% | val_loss=1.6457 acc@1=56.68% | time=344.9s\n",
      "  Epoch 022/100 | train_loss=2.1035 acc=43.35% | val_loss=1.6497 acc@1=56.26% | time=333.1s\n",
      "  Epoch 023/100 | train_loss=2.0820 acc=43.86% | val_loss=1.5610 acc@1=57.52% | time=350.7s\n",
      "  Epoch 024/100 | train_loss=2.1067 acc=43.67% | val_loss=1.5246 acc@1=59.14% | time=333.5s\n",
      "  Epoch 025/100 | train_loss=2.0852 acc=44.93% | val_loss=1.5683 acc@1=58.56% | time=319.7s\n",
      "  Epoch 026/100 | train_loss=2.0954 acc=42.96% | val_loss=1.6081 acc@1=56.60% | time=305.3s\n",
      "  Epoch 027/100 | train_loss=2.0867 acc=44.00% | val_loss=1.6042 acc@1=57.76% | time=307.9s\n",
      "  Epoch 028/100 | train_loss=2.0413 acc=44.66% | val_loss=1.6417 acc@1=56.24% | time=310.7s\n",
      "  Epoch 029/100 | train_loss=2.0540 acc=44.46% | val_loss=1.5341 acc@1=58.42% | time=351.3s\n",
      "  Epoch 030/100 | train_loss=2.0437 acc=45.23% | val_loss=1.5653 acc@1=57.94% | time=304.3s\n",
      "  Epoch 031/100 | train_loss=2.0296 acc=46.17% | val_loss=1.5711 acc@1=58.06% | time=301.4s\n",
      "  Epoch 032/100 | train_loss=1.9947 acc=44.82% | val_loss=1.6327 acc@1=56.78% | time=304.0s\n",
      "  Epoch 033/100 | train_loss=1.9752 acc=46.59% | val_loss=1.5613 acc@1=58.96% | time=302.9s\n",
      "  Epoch 034/100 | train_loss=2.0096 acc=46.32% | val_loss=1.4780 acc@1=60.16% | time=394.3s\n",
      "  Epoch 035/100 | train_loss=2.0026 acc=46.13% | val_loss=1.4874 acc@1=61.26% | time=397.5s\n",
      "  Epoch 036/100 | train_loss=1.9778 acc=46.28% | val_loss=1.5016 acc@1=59.48% | time=380.8s\n",
      "  Epoch 037/100 | train_loss=1.9673 acc=46.99% | val_loss=1.4961 acc@1=59.08% | time=449.6s\n",
      "  Epoch 038/100 | train_loss=1.9723 acc=46.52% | val_loss=1.5566 acc@1=58.16% | time=512.9s\n",
      "  Epoch 039/100 | train_loss=1.9441 acc=47.55% | val_loss=1.4274 acc@1=61.64% | time=506.2s\n",
      "  Epoch 040/100 | train_loss=1.9296 acc=46.51% | val_loss=1.5004 acc@1=60.06% | time=385.8s\n",
      "  Epoch 041/100 | train_loss=1.9331 acc=47.46% | val_loss=1.4804 acc@1=59.98% | time=360.1s\n",
      "  Epoch 042/100 | train_loss=1.9466 acc=47.85% | val_loss=1.4608 acc@1=60.00% | time=358.8s\n",
      "  Epoch 043/100 | train_loss=1.8837 acc=49.09% | val_loss=1.4229 acc@1=62.08% | time=372.0s\n",
      "  Epoch 044/100 | train_loss=1.8828 acc=48.33% | val_loss=1.4477 acc@1=61.02% | time=380.4s\n",
      "  Epoch 045/100 | train_loss=1.8245 acc=49.26% | val_loss=1.4118 acc@1=62.10% | time=447.4s\n",
      "  Epoch 046/100 | train_loss=1.8384 acc=50.05% | val_loss=1.4947 acc@1=59.72% | time=397.3s\n",
      "  Epoch 047/100 | train_loss=1.8502 acc=49.96% | val_loss=1.4359 acc@1=61.14% | time=380.0s\n",
      "  Epoch 048/100 | train_loss=1.8274 acc=49.60% | val_loss=1.3651 acc@1=63.78% | time=387.8s\n",
      "  Epoch 049/100 | train_loss=1.8382 acc=49.14% | val_loss=1.3464 acc@1=63.70% | time=378.9s\n",
      "  Epoch 050/100 | train_loss=1.8021 acc=50.34% | val_loss=1.2728 acc@1=65.24% | time=393.8s\n",
      "  Epoch 051/100 | train_loss=1.7839 acc=51.66% | val_loss=1.3155 acc@1=64.74% | time=492.6s\n",
      "  Epoch 052/100 | train_loss=1.7684 acc=51.81% | val_loss=1.3606 acc@1=64.10% | time=376.3s\n",
      "  Epoch 053/100 | train_loss=1.7782 acc=50.56% | val_loss=1.3276 acc@1=64.24% | time=408.9s\n",
      "  Epoch 054/100 | train_loss=1.7541 acc=52.33% | val_loss=1.3050 acc@1=63.88% | time=378.0s\n",
      "  Epoch 055/100 | train_loss=1.7013 acc=53.26% | val_loss=1.2687 acc@1=65.60% | time=377.9s\n",
      "  Epoch 056/100 | train_loss=1.7202 acc=53.11% | val_loss=1.2731 acc@1=65.58% | time=378.8s\n",
      "  Epoch 057/100 | train_loss=1.6314 acc=54.59% | val_loss=1.2507 acc@1=66.00% | time=382.0s\n",
      "  Epoch 058/100 | train_loss=1.6795 acc=54.49% | val_loss=1.2547 acc@1=66.24% | time=387.2s\n",
      "  Epoch 059/100 | train_loss=1.6632 acc=54.31% | val_loss=1.2428 acc@1=66.68% | time=463.2s\n",
      "  Epoch 060/100 | train_loss=1.6390 acc=55.67% | val_loss=1.2368 acc@1=66.52% | time=480.1s\n",
      "  Epoch 061/100 | train_loss=1.6236 acc=55.61% | val_loss=1.2064 acc@1=67.52% | time=327.3s\n",
      "  Epoch 062/100 | train_loss=1.5978 acc=56.36% | val_loss=1.2195 acc@1=67.18% | time=346.9s\n",
      "  Epoch 063/100 | train_loss=1.5855 acc=54.52% | val_loss=1.1826 acc@1=67.94% | time=355.2s\n",
      "  Epoch 064/100 | train_loss=1.5667 acc=54.87% | val_loss=1.1962 acc@1=67.96% | time=440.4s\n",
      "  Epoch 065/100 | train_loss=1.5653 acc=58.27% | val_loss=1.1834 acc@1=68.00% | time=350.6s\n",
      "  Epoch 066/100 | train_loss=1.5273 acc=57.33% | val_loss=1.1451 acc@1=69.00% | time=306.3s\n",
      "  Epoch 067/100 | train_loss=1.4888 acc=58.64% | val_loss=1.1121 acc@1=70.08% | time=401.1s\n",
      "  Epoch 068/100 | train_loss=1.4748 acc=59.37% | val_loss=1.1420 acc@1=68.96% | time=400.5s\n",
      "  Epoch 069/100 | train_loss=1.4617 acc=59.45% | val_loss=1.0881 acc@1=70.78% | time=422.7s\n",
      "  Epoch 070/100 | train_loss=1.4220 acc=60.84% | val_loss=1.0719 acc@1=70.70% | time=364.3s\n",
      "  Epoch 071/100 | train_loss=1.4096 acc=59.31% | val_loss=1.0515 acc@1=71.50% | time=424.8s\n",
      "  Epoch 072/100 | train_loss=1.3908 acc=60.38% | val_loss=1.0482 acc@1=71.56% | time=477.1s\n",
      "  Epoch 073/100 | train_loss=1.3218 acc=62.12% | val_loss=1.0934 acc@1=70.68% | time=438.0s\n",
      "  Epoch 074/100 | train_loss=1.3615 acc=62.42% | val_loss=1.0182 acc@1=72.70% | time=404.0s\n",
      "  Epoch 075/100 | train_loss=1.3010 acc=63.00% | val_loss=1.0317 acc@1=72.18% | time=483.8s\n",
      "  Epoch 076/100 | train_loss=1.2806 acc=63.66% | val_loss=1.0299 acc@1=72.16% | time=423.0s\n",
      "  Epoch 077/100 | train_loss=1.2499 acc=65.18% | val_loss=1.0631 acc@1=71.60% | time=503.8s\n",
      "  Epoch 078/100 | train_loss=1.1971 acc=66.00% | val_loss=0.9991 acc@1=73.26% | time=382.7s\n",
      "  Epoch 079/100 | train_loss=1.2206 acc=64.79% | val_loss=0.9456 acc@1=74.54% | time=380.8s\n",
      "  Epoch 080/100 | train_loss=1.1967 acc=66.60% | val_loss=0.9700 acc@1=74.06% | time=467.0s\n",
      "  Epoch 081/100 | train_loss=1.1499 acc=65.75% | val_loss=0.9552 acc@1=74.36% | time=361.3s\n",
      "  Epoch 082/100 | train_loss=1.1525 acc=66.15% | val_loss=0.9228 acc@1=75.14% | time=380.7s\n",
      "  Epoch 083/100 | train_loss=1.1605 acc=65.49% | val_loss=0.9140 acc@1=75.20% | time=397.2s\n",
      "  Epoch 084/100 | train_loss=1.0923 acc=69.20% | val_loss=0.9182 acc@1=76.00% | time=395.6s\n",
      "  Epoch 085/100 | train_loss=1.0731 acc=67.72% | val_loss=0.9165 acc@1=75.70% | time=313.4s\n",
      "  Epoch 086/100 | train_loss=1.0904 acc=68.91% | val_loss=0.8948 acc@1=75.72% | time=445.4s\n",
      "  Epoch 087/100 | train_loss=0.9918 acc=70.90% | val_loss=0.8717 acc@1=76.76% | time=469.2s\n",
      "  Epoch 088/100 | train_loss=0.9942 acc=69.47% | val_loss=0.8814 acc@1=76.10% | time=353.8s\n",
      "  Epoch 089/100 | train_loss=1.0245 acc=70.63% | val_loss=0.8571 acc@1=77.20% | time=336.0s\n",
      "  Epoch 090/100 | train_loss=1.0118 acc=70.72% | val_loss=0.8925 acc@1=77.04% | time=335.1s\n",
      "  Epoch 091/100 | train_loss=0.9672 acc=71.21% | val_loss=0.8486 acc@1=77.96% | time=352.5s\n",
      "  Epoch 092/100 | train_loss=0.9804 acc=71.99% | val_loss=0.8367 acc@1=77.42% | time=343.0s\n",
      "  Epoch 093/100 | train_loss=0.9003 acc=73.22% | val_loss=0.8272 acc@1=77.66% | time=337.1s\n",
      "  Epoch 094/100 | train_loss=0.9563 acc=70.14% | val_loss=0.8353 acc@1=77.88% | time=336.3s\n",
      "  Epoch 095/100 | train_loss=0.9483 acc=72.73% | val_loss=0.8577 acc@1=77.76% | time=347.8s\n",
      "  Epoch 096/100 | train_loss=0.9652 acc=70.60% | val_loss=0.8344 acc@1=77.38% | time=335.4s\n",
      "  Epoch 097/100 | train_loss=0.8941 acc=72.91% | val_loss=0.8403 acc@1=77.86% | time=390.4s\n",
      "  Epoch 098/100 | train_loss=0.9717 acc=71.34% | val_loss=0.8415 acc@1=77.82% | time=340.8s\n",
      "  Epoch 099/100 | train_loss=0.9428 acc=73.38% | val_loss=0.8226 acc@1=77.96% | time=301.9s\n",
      "  Epoch 100/100 | train_loss=0.9488 acc=69.70% | val_loss=0.8249 acc@1=77.74% | time=299.5s\n",
      "  -> Best checkpoint: ./outputs\\resnet18_e100_bs32_acc2_20251201-131328\\checkpoints\\resnet18_best.pt\n",
      "  -> Plots: ./outputs\\resnet18_e100_bs32_acc2_20251201-131328\\plots\n",
      "  -> Results: ./outputs\\resnet18_e100_bs32_acc2_20251201-131328\\results\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'resnet18',\n",
       " 'epochs_run': 100,\n",
       " 'optimizer': 'sgd',\n",
       " 'lr_final': 0.0,\n",
       " 'weight_decay': 0.0005,\n",
       " 'batch_size': 32,\n",
       " 'accum_steps': 2,\n",
       " 'max_grad_norm': None,\n",
       " 'label_smoothing': 0.0,\n",
       " 'mixup_cutmix': {'enabled': True,\n",
       "  'prob': 0.6,\n",
       "  'mixup_alpha': 0.2,\n",
       "  'cutmix_alpha': 1.0,\n",
       "  'switch_prob': 0.5},\n",
       " 'ema': {'enabled': True, 'decay': 0.9999, 'eval': False},\n",
       " 'val': {'loss': 0.853527719783783, 'top1': 0.7734, 'top5': 0.9434},\n",
       " 'test': {'loss': 0.7094504383087158, 'top1': 0.8057, 'top5': 0.9611},\n",
       " 'ece_raw': 0.022068275982141495,\n",
       " 'ece_temp_scaled': 0.028767734242975712,\n",
       " 'temperature': 0.8810027241706848,\n",
       " 'checkpoint': './outputs\\\\resnet18_e100_bs32_acc2_20251201-131328\\\\checkpoints\\\\resnet18_best.pt',\n",
       " 'run_dir': './outputs\\\\resnet18_e100_bs32_acc2_20251201-131328'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_metrics = train_and_evaluate_one_model('resnet18')\n",
    "resnet_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed0ab40",
   "metadata": {},
   "source": [
    "## 12) Train: WideResNet‑28×10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22d3c31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "==> [wrn28x10] Training for 160 epochs | opt=sgd lr=0.1 wd=0.0005 | img=32 | bs=128 | accum=1\n",
      "  Epoch 001/160 | train_loss=4.4175 acc=3.67% | val_loss=4.1333 acc@1=7.14% | time=364.3s\n",
      "  Epoch 002/160 | train_loss=4.0402 acc=7.73% | val_loss=3.7964 acc@1=11.38% | time=361.9s\n",
      "  Epoch 003/160 | train_loss=3.7202 acc=11.97% | val_loss=3.3823 acc@1=17.32% | time=362.2s\n",
      "  Epoch 004/160 | train_loss=3.4620 acc=16.03% | val_loss=3.1923 acc@1=21.86% | time=371.0s\n",
      "  Epoch 005/160 | train_loss=3.1651 acc=21.35% | val_loss=3.4961 acc@1=20.02% | time=365.6s\n",
      "  Epoch 006/160 | train_loss=3.0181 acc=24.65% | val_loss=2.8543 acc@1=28.20% | time=363.9s\n",
      "  Epoch 007/160 | train_loss=2.8574 acc=25.85% | val_loss=2.5934 acc@1=33.62% | time=388.2s\n",
      "  Epoch 008/160 | train_loss=2.7101 acc=29.85% | val_loss=2.5102 acc@1=35.74% | time=385.3s\n",
      "  Epoch 009/160 | train_loss=2.6512 acc=31.62% | val_loss=2.4561 acc@1=36.12% | time=381.7s\n",
      "  Epoch 010/160 | train_loss=2.6009 acc=32.77% | val_loss=2.2898 acc@1=40.10% | time=380.2s\n",
      "  Epoch 011/160 | train_loss=2.5437 acc=33.94% | val_loss=2.3289 acc@1=39.50% | time=365.0s\n",
      "  Epoch 012/160 | train_loss=2.4477 acc=34.09% | val_loss=2.3222 acc@1=39.40% | time=366.3s\n",
      "  Epoch 013/160 | train_loss=2.4289 acc=34.98% | val_loss=2.4715 acc@1=36.56% | time=364.8s\n",
      "  Epoch 014/160 | train_loss=2.4110 acc=36.29% | val_loss=2.2710 acc@1=42.30% | time=366.2s\n",
      "  Epoch 015/160 | train_loss=2.3441 acc=37.18% | val_loss=2.1447 acc@1=43.54% | time=384.6s\n",
      "  Epoch 016/160 | train_loss=2.3295 acc=39.21% | val_loss=2.3969 acc@1=38.46% | time=386.0s\n",
      "  Epoch 017/160 | train_loss=2.2893 acc=38.63% | val_loss=2.2499 acc@1=40.66% | time=429.5s\n",
      "  Epoch 018/160 | train_loss=2.2495 acc=38.78% | val_loss=2.1516 acc@1=44.08% | time=383.1s\n",
      "  Epoch 019/160 | train_loss=2.2563 acc=39.46% | val_loss=2.1927 acc@1=43.78% | time=380.6s\n",
      "  Epoch 020/160 | train_loss=2.2110 acc=40.62% | val_loss=2.0819 acc@1=45.62% | time=381.1s\n",
      "  Epoch 021/160 | train_loss=2.1899 acc=41.41% | val_loss=1.7965 acc@1=51.42% | time=382.5s\n",
      "  Epoch 022/160 | train_loss=2.1541 acc=42.40% | val_loss=1.9061 acc@1=48.94% | time=379.3s\n",
      "  Epoch 023/160 | train_loss=2.1387 acc=43.91% | val_loss=2.1888 acc@1=43.38% | time=379.4s\n",
      "  Epoch 024/160 | train_loss=2.1717 acc=41.94% | val_loss=1.9731 acc@1=47.34% | time=380.9s\n",
      "  Epoch 025/160 | train_loss=2.1539 acc=42.58% | val_loss=2.0189 acc@1=47.08% | time=383.0s\n",
      "  Epoch 026/160 | train_loss=2.1018 acc=43.34% | val_loss=2.0667 acc@1=46.06% | time=392.3s\n",
      "  Epoch 027/160 | train_loss=2.1165 acc=44.26% | val_loss=2.1907 acc@1=44.86% | time=382.1s\n",
      "  Epoch 028/160 | train_loss=2.1185 acc=44.57% | val_loss=2.3926 acc@1=41.60% | time=382.2s\n",
      "  Epoch 029/160 | train_loss=2.1317 acc=42.85% | val_loss=1.9340 acc@1=48.84% | time=383.4s\n",
      "  Epoch 030/160 | train_loss=2.0636 acc=42.27% | val_loss=1.9034 acc@1=49.26% | time=383.9s\n",
      "  Epoch 031/160 | train_loss=2.1540 acc=41.93% | val_loss=1.7871 acc@1=52.56% | time=382.1s\n",
      "  Epoch 032/160 | train_loss=2.0779 acc=42.53% | val_loss=2.0529 acc@1=47.18% | time=380.0s\n",
      "  Epoch 033/160 | train_loss=2.0864 acc=43.49% | val_loss=1.9252 acc@1=49.78% | time=382.7s\n",
      "  Epoch 034/160 | train_loss=2.0021 acc=46.36% | val_loss=2.0102 acc@1=47.46% | time=380.1s\n",
      "  Epoch 035/160 | train_loss=2.0034 acc=46.45% | val_loss=1.9655 acc@1=48.18% | time=393.7s\n",
      "  Epoch 036/160 | train_loss=1.9937 acc=46.36% | val_loss=1.7518 acc@1=52.48% | time=395.1s\n",
      "  Epoch 037/160 | train_loss=2.0040 acc=46.91% | val_loss=1.8167 acc@1=51.60% | time=383.8s\n",
      "  Epoch 038/160 | train_loss=1.9638 acc=46.08% | val_loss=1.8848 acc@1=49.62% | time=377.9s\n",
      "  Epoch 039/160 | train_loss=1.9460 acc=47.97% | val_loss=1.6978 acc@1=55.30% | time=363.0s\n",
      "  Epoch 040/160 | train_loss=1.9719 acc=45.62% | val_loss=1.7393 acc@1=54.06% | time=378.8s\n",
      "  Epoch 041/160 | train_loss=1.9793 acc=46.22% | val_loss=1.9120 acc@1=50.78% | time=378.6s\n",
      "  Epoch 042/160 | train_loss=1.9455 acc=44.93% | val_loss=1.5348 acc@1=57.90% | time=385.1s\n",
      "  Epoch 043/160 | train_loss=1.9952 acc=45.94% | val_loss=1.7833 acc@1=52.86% | time=384.0s\n",
      "  Epoch 044/160 | train_loss=1.9175 acc=47.89% | val_loss=1.6937 acc@1=54.76% | time=383.8s\n",
      "  Epoch 045/160 | train_loss=1.9105 acc=47.65% | val_loss=2.0816 acc@1=48.00% | time=397.8s\n",
      "  Epoch 046/160 | train_loss=1.9723 acc=47.68% | val_loss=1.5613 acc@1=58.16% | time=380.7s\n",
      "  Epoch 047/160 | train_loss=1.9313 acc=46.62% | val_loss=1.7872 acc@1=52.76% | time=381.9s\n",
      "  Epoch 048/160 | train_loss=1.8599 acc=48.17% | val_loss=1.7572 acc@1=53.54% | time=379.2s\n",
      "  Epoch 049/160 | train_loss=1.8998 acc=48.66% | val_loss=1.5791 acc@1=57.78% | time=377.6s\n",
      "  Epoch 050/160 | train_loss=1.8499 acc=48.77% | val_loss=1.7152 acc@1=53.76% | time=364.8s\n",
      "  Epoch 051/160 | train_loss=1.8698 acc=49.96% | val_loss=1.6648 acc@1=55.44% | time=364.8s\n",
      "  Epoch 052/160 | train_loss=1.8539 acc=48.27% | val_loss=1.6159 acc@1=56.46% | time=361.7s\n",
      "  Epoch 053/160 | train_loss=1.8356 acc=50.17% | val_loss=1.8134 acc@1=53.24% | time=363.8s\n",
      "  Epoch 054/160 | train_loss=1.8533 acc=48.32% | val_loss=1.6027 acc@1=57.24% | time=379.1s\n",
      "  Epoch 055/160 | train_loss=1.7874 acc=49.98% | val_loss=1.6197 acc@1=56.74% | time=399.6s\n",
      "  Epoch 056/160 | train_loss=1.8003 acc=48.79% | val_loss=1.5711 acc@1=57.74% | time=392.6s\n",
      "  Epoch 057/160 | train_loss=1.8384 acc=48.76% | val_loss=1.5911 acc@1=57.24% | time=382.9s\n",
      "  Epoch 058/160 | train_loss=1.8463 acc=49.18% | val_loss=1.7173 acc@1=55.98% | time=380.4s\n",
      "  Epoch 059/160 | train_loss=1.8035 acc=51.55% | val_loss=1.5478 acc@1=58.70% | time=379.1s\n",
      "  Epoch 060/160 | train_loss=1.7706 acc=50.46% | val_loss=1.5298 acc@1=58.20% | time=380.4s\n",
      "  Epoch 061/160 | train_loss=1.8301 acc=51.00% | val_loss=1.5660 acc@1=57.52% | time=377.4s\n",
      "  Epoch 062/160 | train_loss=1.7997 acc=49.59% | val_loss=1.6147 acc@1=56.06% | time=382.5s\n",
      "  Epoch 063/160 | train_loss=1.8031 acc=51.71% | val_loss=1.5751 acc@1=56.48% | time=379.9s\n",
      "  Epoch 064/160 | train_loss=1.8114 acc=52.26% | val_loss=1.6623 acc@1=55.50% | time=398.4s\n",
      "  Epoch 065/160 | train_loss=1.7663 acc=49.85% | val_loss=1.4749 acc@1=60.70% | time=381.9s\n",
      "  Epoch 066/160 | train_loss=1.8023 acc=50.97% | val_loss=1.5584 acc@1=58.36% | time=377.8s\n",
      "  Epoch 067/160 | train_loss=1.7660 acc=51.63% | val_loss=1.7795 acc@1=53.54% | time=380.0s\n",
      "  Epoch 068/160 | train_loss=1.7548 acc=49.24% | val_loss=1.4991 acc@1=59.60% | time=379.7s\n",
      "  Epoch 069/160 | train_loss=1.7998 acc=50.41% | val_loss=1.5408 acc@1=58.72% | time=387.5s\n",
      "  Epoch 070/160 | train_loss=1.6478 acc=53.62% | val_loss=1.5160 acc@1=59.48% | time=381.9s\n",
      "  Epoch 071/160 | train_loss=1.7094 acc=52.46% | val_loss=1.4312 acc@1=60.88% | time=379.5s\n",
      "  Epoch 072/160 | train_loss=1.7407 acc=52.44% | val_loss=1.4972 acc@1=60.06% | time=380.2s\n",
      "  Epoch 073/160 | train_loss=1.6365 acc=54.96% | val_loss=1.4088 acc@1=61.60% | time=376.1s\n",
      "  Epoch 074/160 | train_loss=1.7471 acc=51.82% | val_loss=1.3645 acc@1=62.32% | time=403.2s\n",
      "  Epoch 075/160 | train_loss=1.6996 acc=51.38% | val_loss=1.5186 acc@1=59.60% | time=376.4s\n",
      "  Epoch 076/160 | train_loss=1.7431 acc=54.28% | val_loss=1.4953 acc@1=59.84% | time=373.4s\n",
      "  Epoch 077/160 | train_loss=1.6721 acc=54.91% | val_loss=1.5450 acc@1=58.74% | time=382.0s\n",
      "  Epoch 078/160 | train_loss=1.6940 acc=53.10% | val_loss=1.4031 acc@1=62.10% | time=378.4s\n",
      "  Epoch 079/160 | train_loss=1.6316 acc=54.69% | val_loss=1.5002 acc@1=60.44% | time=382.3s\n",
      "  Epoch 080/160 | train_loss=1.6314 acc=54.22% | val_loss=1.3918 acc@1=62.68% | time=384.4s\n",
      "  Epoch 081/160 | train_loss=1.6009 acc=56.31% | val_loss=1.3931 acc@1=62.08% | time=378.3s\n",
      "  Epoch 082/160 | train_loss=1.5618 acc=55.29% | val_loss=1.4493 acc@1=61.56% | time=380.3s\n",
      "  Epoch 083/160 | train_loss=1.6315 acc=53.77% | val_loss=1.4539 acc@1=60.78% | time=383.0s\n",
      "  Epoch 084/160 | train_loss=1.5676 acc=54.06% | val_loss=1.3390 acc@1=63.14% | time=397.4s\n",
      "  Epoch 085/160 | train_loss=1.4535 acc=58.20% | val_loss=1.3545 acc@1=63.30% | time=378.8s\n",
      "  Epoch 086/160 | train_loss=1.6513 acc=54.36% | val_loss=1.3935 acc@1=62.26% | time=375.5s\n",
      "  Epoch 087/160 | train_loss=1.5519 acc=56.41% | val_loss=1.3369 acc@1=64.06% | time=377.0s\n",
      "  Epoch 088/160 | train_loss=1.4838 acc=57.71% | val_loss=1.3601 acc@1=63.56% | time=375.4s\n",
      "  Epoch 089/160 | train_loss=1.4730 acc=60.13% | val_loss=1.3924 acc@1=62.24% | time=386.9s\n",
      "  Epoch 090/160 | train_loss=1.4659 acc=60.47% | val_loss=1.3775 acc@1=62.94% | time=378.6s\n",
      "  Epoch 091/160 | train_loss=1.5601 acc=58.16% | val_loss=1.3384 acc@1=64.36% | time=369.1s\n",
      "  Epoch 092/160 | train_loss=1.4754 acc=58.78% | val_loss=1.3400 acc@1=64.24% | time=382.9s\n",
      "  Epoch 093/160 | train_loss=1.4714 acc=55.78% | val_loss=1.3350 acc@1=63.84% | time=452.7s\n",
      "  Epoch 094/160 | train_loss=1.4793 acc=59.47% | val_loss=1.2213 acc@1=66.48% | time=446.7s\n",
      "  Epoch 095/160 | train_loss=1.3420 acc=60.77% | val_loss=1.2810 acc@1=65.24% | time=368.2s\n",
      "  Epoch 096/160 | train_loss=1.3956 acc=60.04% | val_loss=1.2283 acc@1=66.56% | time=366.2s\n",
      "  Epoch 097/160 | train_loss=1.4548 acc=59.71% | val_loss=1.2143 acc@1=67.06% | time=367.3s\n",
      "  Epoch 098/160 | train_loss=1.4066 acc=59.30% | val_loss=1.2272 acc@1=66.66% | time=364.9s\n",
      "  Epoch 099/160 | train_loss=1.3266 acc=59.47% | val_loss=1.2154 acc@1=66.90% | time=387.7s\n",
      "  Epoch 100/160 | train_loss=1.3916 acc=59.94% | val_loss=1.1794 acc@1=67.86% | time=395.9s\n",
      "  Epoch 101/160 | train_loss=1.3043 acc=64.05% | val_loss=1.2711 acc@1=66.58% | time=385.5s\n",
      "  Epoch 102/160 | train_loss=1.3003 acc=61.57% | val_loss=1.2456 acc@1=66.66% | time=383.9s\n",
      "  Epoch 103/160 | train_loss=1.2649 acc=62.90% | val_loss=1.2558 acc@1=66.40% | time=391.4s\n",
      "  Epoch 104/160 | train_loss=1.2218 acc=63.35% | val_loss=1.2800 acc@1=65.74% | time=410.2s\n",
      "  Epoch 105/160 | train_loss=1.2497 acc=64.01% | val_loss=1.1703 acc@1=68.46% | time=396.3s\n",
      "  Epoch 106/160 | train_loss=1.2482 acc=63.25% | val_loss=1.1425 acc@1=69.16% | time=394.0s\n",
      "  Epoch 107/160 | train_loss=1.3413 acc=61.95% | val_loss=1.1120 acc@1=69.54% | time=432.5s\n",
      "  Epoch 108/160 | train_loss=1.3023 acc=61.63% | val_loss=1.1101 acc@1=69.48% | time=447.1s\n",
      "  Epoch 109/160 | train_loss=1.1897 acc=63.31% | val_loss=1.2109 acc@1=67.84% | time=450.1s\n",
      "  Epoch 110/160 | train_loss=1.2582 acc=61.61% | val_loss=1.0905 acc@1=69.52% | time=394.1s\n",
      "  Epoch 111/160 | train_loss=1.1828 acc=67.40% | val_loss=1.1914 acc@1=68.90% | time=438.5s\n",
      "  Epoch 112/160 | train_loss=1.1578 acc=66.16% | val_loss=1.0618 acc@1=70.88% | time=402.1s\n",
      "  Epoch 113/160 | train_loss=1.0973 acc=67.18% | val_loss=1.0740 acc@1=70.34% | time=416.3s\n",
      "  Epoch 114/160 | train_loss=1.1074 acc=68.99% | val_loss=1.0407 acc@1=71.86% | time=366.4s\n",
      "  Epoch 115/160 | train_loss=1.0958 acc=68.11% | val_loss=1.1025 acc@1=70.20% | time=370.6s\n",
      "  Epoch 116/160 | train_loss=1.0805 acc=68.06% | val_loss=1.0183 acc@1=72.32% | time=397.2s\n",
      "  Epoch 117/160 | train_loss=1.0743 acc=67.58% | val_loss=1.0339 acc@1=71.84% | time=390.1s\n",
      "  Epoch 118/160 | train_loss=1.0551 acc=69.34% | val_loss=1.0093 acc@1=72.52% | time=410.2s\n",
      "  Epoch 119/160 | train_loss=1.0138 acc=69.16% | val_loss=1.0298 acc@1=71.48% | time=461.2s\n",
      "  Epoch 120/160 | train_loss=1.0581 acc=70.99% | val_loss=1.0691 acc@1=71.36% | time=461.1s\n",
      "  Epoch 121/160 | train_loss=1.0189 acc=69.54% | val_loss=1.0166 acc@1=73.02% | time=388.1s\n",
      "  Epoch 122/160 | train_loss=1.0054 acc=69.69% | val_loss=1.0650 acc@1=71.68% | time=377.2s\n",
      "  Epoch 123/160 | train_loss=0.9509 acc=72.50% | val_loss=1.0210 acc@1=72.68% | time=374.6s\n",
      "  Epoch 124/160 | train_loss=1.1103 acc=67.93% | val_loss=0.9982 acc@1=73.16% | time=383.8s\n",
      "  Epoch 125/160 | train_loss=1.0291 acc=70.98% | val_loss=0.9816 acc@1=73.82% | time=394.8s\n",
      "  Epoch 126/160 | train_loss=1.0143 acc=72.60% | val_loss=0.9647 acc@1=74.36% | time=379.7s\n",
      "  Epoch 127/160 | train_loss=0.9135 acc=73.80% | val_loss=0.9158 acc@1=75.00% | time=389.1s\n",
      "  Epoch 128/160 | train_loss=0.9294 acc=73.41% | val_loss=0.9336 acc@1=75.60% | time=451.2s\n",
      "  Epoch 129/160 | train_loss=0.9279 acc=71.22% | val_loss=0.9424 acc@1=74.92% | time=398.3s\n",
      "  Epoch 130/160 | train_loss=0.9328 acc=72.31% | val_loss=0.8926 acc@1=76.00% | time=409.0s\n",
      "  Epoch 131/160 | train_loss=0.9665 acc=71.23% | val_loss=0.9672 acc@1=74.42% | time=403.1s\n",
      "  Epoch 132/160 | train_loss=1.0242 acc=68.15% | val_loss=0.9034 acc@1=76.08% | time=438.9s\n",
      "  Epoch 133/160 | train_loss=0.8101 acc=75.08% | val_loss=0.8551 acc@1=77.36% | time=456.6s\n",
      "  Epoch 134/160 | train_loss=0.7878 acc=74.21% | val_loss=0.8893 acc@1=76.30% | time=389.5s\n",
      "  Epoch 135/160 | train_loss=0.7973 acc=76.07% | val_loss=0.8631 acc@1=77.28% | time=442.8s\n",
      "  Epoch 136/160 | train_loss=0.7772 acc=76.99% | val_loss=0.8554 acc@1=77.40% | time=468.2s\n",
      "  Epoch 137/160 | train_loss=0.7681 acc=77.23% | val_loss=0.8453 acc@1=77.64% | time=395.2s\n",
      "  Epoch 138/160 | train_loss=0.7981 acc=75.68% | val_loss=0.8692 acc@1=76.92% | time=367.8s\n",
      "  Epoch 139/160 | train_loss=0.7370 acc=75.77% | val_loss=0.8494 acc@1=77.16% | time=395.6s\n",
      "  Epoch 140/160 | train_loss=0.7921 acc=74.22% | val_loss=0.8310 acc@1=77.68% | time=426.9s\n",
      "  Epoch 141/160 | train_loss=0.7590 acc=73.13% | val_loss=0.8549 acc@1=77.96% | time=387.0s\n",
      "  Epoch 142/160 | train_loss=0.6998 acc=78.06% | val_loss=0.8333 acc@1=77.80% | time=400.7s\n",
      "  Epoch 143/160 | train_loss=0.7318 acc=74.85% | val_loss=0.8182 acc@1=78.40% | time=407.4s\n",
      "  Epoch 144/160 | train_loss=0.7888 acc=73.12% | val_loss=0.8405 acc@1=78.20% | time=383.0s\n",
      "  Epoch 145/160 | train_loss=0.7987 acc=73.59% | val_loss=0.8021 acc@1=79.14% | time=382.1s\n",
      "  Epoch 146/160 | train_loss=0.7848 acc=76.10% | val_loss=0.8124 acc@1=78.40% | time=383.7s\n",
      "  Epoch 147/160 | train_loss=0.7193 acc=78.72% | val_loss=0.7913 acc@1=79.58% | time=378.9s\n",
      "  Epoch 148/160 | train_loss=0.6040 acc=80.59% | val_loss=0.7696 acc@1=79.42% | time=378.1s\n",
      "  Epoch 149/160 | train_loss=0.6605 acc=75.31% | val_loss=0.7864 acc@1=79.34% | time=384.8s\n",
      "  Epoch 150/160 | train_loss=0.6174 acc=77.05% | val_loss=0.7700 acc@1=79.46% | time=392.0s\n",
      "  Epoch 151/160 | train_loss=0.7261 acc=77.78% | val_loss=0.7831 acc@1=79.82% | time=388.6s\n",
      "  Epoch 152/160 | train_loss=0.6356 acc=78.31% | val_loss=0.7713 acc@1=79.88% | time=386.2s\n",
      "  Epoch 153/160 | train_loss=0.6556 acc=77.98% | val_loss=0.7886 acc@1=79.24% | time=387.7s\n",
      "  Epoch 154/160 | train_loss=0.7815 acc=78.36% | val_loss=0.7830 acc@1=79.64% | time=389.3s\n",
      "  Epoch 155/160 | train_loss=0.7237 acc=79.07% | val_loss=0.7666 acc@1=79.82% | time=388.2s\n",
      "  Epoch 156/160 | train_loss=0.6842 acc=77.21% | val_loss=0.7593 acc@1=80.00% | time=413.9s\n",
      "  Epoch 157/160 | train_loss=0.6760 acc=80.91% | val_loss=0.7536 acc@1=80.28% | time=395.3s\n",
      "  Epoch 158/160 | train_loss=0.6724 acc=79.58% | val_loss=0.7703 acc@1=79.60% | time=402.5s\n",
      "  Epoch 159/160 | train_loss=0.6380 acc=79.57% | val_loss=0.7826 acc@1=79.66% | time=370.6s\n",
      "  Epoch 160/160 | train_loss=0.7141 acc=78.54% | val_loss=0.7590 acc@1=79.66% | time=366.6s\n",
      "  -> Best checkpoint: ./outputs\\wrn28x10_e160_bs128_acc1_20251201-235123\\checkpoints\\wrn28x10_best.pt\n",
      "  -> Plots: ./outputs\\wrn28x10_e160_bs128_acc1_20251201-235123\\plots\n",
      "  -> Results: ./outputs\\wrn28x10_e160_bs128_acc1_20251201-235123\\results\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'wrn28x10',\n",
       " 'epochs_run': 160,\n",
       " 'optimizer': 'sgd',\n",
       " 'lr_final': 0.0,\n",
       " 'weight_decay': 0.0005,\n",
       " 'batch_size': 128,\n",
       " 'accum_steps': 1,\n",
       " 'max_grad_norm': None,\n",
       " 'label_smoothing': 0.0,\n",
       " 'mixup_cutmix': {'enabled': True,\n",
       "  'prob': 0.5,\n",
       "  'mixup_alpha': 0.2,\n",
       "  'cutmix_alpha': 1.0,\n",
       "  'switch_prob': 0.5},\n",
       " 'ema': {'enabled': True, 'decay': 0.9999, 'eval': False},\n",
       " 'val': {'loss': 0.761966065955162, 'top1': 0.798, 'top5': 0.9506},\n",
       " 'test': {'loss': 0.6417224734306336, 'top1': 0.8271, 'top5': 0.9636},\n",
       " 'ece_raw': 0.0201254317894578,\n",
       " 'ece_temp_scaled': 0.0291562568038702,\n",
       " 'temperature': 0.9658310413360596,\n",
       " 'checkpoint': './outputs\\\\wrn28x10_e160_bs128_acc1_20251201-235123\\\\checkpoints\\\\wrn28x10_best.pt',\n",
       " 'run_dir': './outputs\\\\wrn28x10_e160_bs128_acc1_20251201-235123'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrn_metrics = train_and_evaluate_one_model('wrn28x10')\n",
    "wrn_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b3cf27",
   "metadata": {},
   "source": [
    "## 13) Train: ConvNeXt‑Tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cbbe072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "==> [convnext_tiny] Training for 80 epochs | opt=adamw lr=0.0002 wd=0.05 | img=224 | bs=32 | accum=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akugupta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 001/80 | train_loss=2.4948 acc=36.75% | val_loss=1.0260 acc@1=73.38% | time=573.1s\n",
      "  Epoch 002/80 | train_loss=1.7750 acc=48.82% | val_loss=1.1076 acc@1=69.92% | time=550.4s\n",
      "  Epoch 003/80 | train_loss=1.7487 acc=48.17% | val_loss=0.9991 acc@1=72.42% | time=577.9s\n",
      "  Epoch 004/80 | train_loss=1.6000 acc=51.83% | val_loss=0.9590 acc@1=73.46% | time=584.8s\n",
      "  Epoch 005/80 | train_loss=1.4756 acc=52.70% | val_loss=0.8885 acc@1=74.98% | time=685.6s\n",
      "  Epoch 006/80 | train_loss=1.3632 acc=56.07% | val_loss=0.8800 acc@1=75.50% | time=682.3s\n",
      "  Epoch 007/80 | train_loss=1.3126 acc=54.63% | val_loss=0.8405 acc@1=76.32% | time=659.5s\n",
      "  Epoch 008/80 | train_loss=1.2640 acc=58.10% | val_loss=0.7986 acc@1=77.44% | time=614.9s\n",
      "  Epoch 009/80 | train_loss=1.2256 acc=59.96% | val_loss=0.7603 acc@1=79.28% | time=716.0s\n",
      "  Epoch 010/80 | train_loss=1.1485 acc=59.11% | val_loss=0.7808 acc@1=79.06% | time=479.7s\n",
      "  Epoch 011/80 | train_loss=1.1240 acc=60.34% | val_loss=0.8376 acc@1=77.70% | time=397.3s\n",
      "  Epoch 012/80 | train_loss=1.0954 acc=61.29% | val_loss=0.8226 acc@1=77.84% | time=433.7s\n",
      "  Epoch 013/80 | train_loss=1.0942 acc=62.70% | val_loss=0.7993 acc@1=78.94% | time=439.9s\n",
      "  Epoch 014/80 | train_loss=1.0553 acc=61.99% | val_loss=0.8231 acc@1=78.32% | time=438.9s\n",
      "  Epoch 015/80 | train_loss=1.0291 acc=62.64% | val_loss=0.8516 acc@1=78.14% | time=436.9s\n",
      "  Epoch 016/80 | train_loss=1.0364 acc=61.23% | val_loss=0.8113 acc@1=79.18% | time=465.6s\n",
      "  Epoch 017/80 | train_loss=1.0228 acc=63.05% | val_loss=0.8078 acc@1=79.74% | time=434.4s\n",
      "  Epoch 018/80 | train_loss=0.9640 acc=64.78% | val_loss=0.8271 acc@1=78.54% | time=544.4s\n",
      "  Epoch 019/80 | train_loss=0.9971 acc=63.00% | val_loss=0.8003 acc@1=79.54% | time=513.7s\n",
      "  Epoch 020/80 | train_loss=0.9504 acc=64.54% | val_loss=0.8821 acc@1=79.10% | time=489.7s\n",
      "  Epoch 021/80 | train_loss=0.9112 acc=63.54% | val_loss=0.8673 acc@1=79.10% | time=451.3s\n",
      "  Epoch 022/80 | train_loss=0.9438 acc=66.29% | val_loss=0.8455 acc@1=79.20% | time=497.4s\n",
      "  Epoch 023/80 | train_loss=0.9339 acc=65.06% | val_loss=0.7812 acc@1=80.44% | time=516.9s\n",
      "  Epoch 024/80 | train_loss=0.9221 acc=64.69% | val_loss=0.8060 acc@1=80.42% | time=398.0s\n",
      "  Epoch 025/80 | train_loss=0.8784 acc=64.02% | val_loss=0.7847 acc@1=80.18% | time=396.7s\n",
      "  Epoch 026/80 | train_loss=0.8639 acc=65.46% | val_loss=0.7947 acc@1=80.36% | time=470.4s\n",
      "  Epoch 027/80 | train_loss=0.8835 acc=65.32% | val_loss=0.8434 acc@1=79.70% | time=473.4s\n",
      "  Epoch 028/80 | train_loss=0.8315 acc=66.96% | val_loss=0.8791 acc@1=80.32% | time=470.2s\n",
      "  Epoch 029/80 | train_loss=0.8636 acc=64.76% | val_loss=0.8782 acc@1=79.72% | time=394.1s\n",
      "  Epoch 030/80 | train_loss=0.8465 acc=66.41% | val_loss=0.8391 acc@1=80.78% | time=492.5s\n",
      "  Epoch 031/80 | train_loss=0.8244 acc=67.21% | val_loss=0.8297 acc@1=81.86% | time=546.8s\n",
      "  Epoch 032/80 | train_loss=0.8110 acc=66.15% | val_loss=0.8185 acc@1=81.54% | time=695.3s\n",
      "  Epoch 033/80 | train_loss=0.8333 acc=66.63% | val_loss=0.8275 acc@1=81.46% | time=607.5s\n",
      "  Epoch 034/80 | train_loss=0.8272 acc=67.00% | val_loss=0.8180 acc@1=81.84% | time=535.8s\n",
      "  Epoch 035/80 | train_loss=0.7813 acc=65.50% | val_loss=0.7992 acc@1=81.90% | time=536.9s\n",
      "  Epoch 036/80 | train_loss=0.7684 acc=68.13% | val_loss=0.7940 acc@1=82.10% | time=580.2s\n",
      "  Epoch 037/80 | train_loss=0.7515 acc=68.49% | val_loss=0.8076 acc@1=81.66% | time=712.5s\n",
      "  Epoch 038/80 | train_loss=0.7706 acc=66.58% | val_loss=0.8424 acc@1=81.92% | time=545.8s\n",
      "  Epoch 039/80 | train_loss=0.7338 acc=67.79% | val_loss=0.8662 acc@1=81.22% | time=470.1s\n",
      "  Epoch 040/80 | train_loss=0.7045 acc=68.06% | val_loss=0.8712 acc@1=81.96% | time=398.6s\n",
      "  Epoch 041/80 | train_loss=0.7467 acc=69.03% | val_loss=0.8509 acc@1=82.48% | time=392.8s\n",
      "  Epoch 042/80 | train_loss=0.7310 acc=69.20% | val_loss=0.8998 acc@1=81.54% | time=391.9s\n",
      "  Epoch 043/80 | train_loss=0.7394 acc=66.88% | val_loss=0.8298 acc@1=82.92% | time=392.6s\n",
      "  Epoch 044/80 | train_loss=0.7017 acc=69.40% | val_loss=0.8372 acc@1=82.56% | time=392.7s\n",
      "  Epoch 045/80 | train_loss=0.7129 acc=69.52% | val_loss=0.8668 acc@1=82.46% | time=390.7s\n",
      "  Epoch 046/80 | train_loss=0.7247 acc=68.03% | val_loss=0.8158 acc@1=83.50% | time=392.2s\n",
      "  Epoch 047/80 | train_loss=0.6894 acc=68.85% | val_loss=0.8596 acc@1=82.88% | time=460.1s\n",
      "  Epoch 048/80 | train_loss=0.6638 acc=71.48% | val_loss=0.8867 acc@1=82.70% | time=427.8s\n",
      "  Epoch 049/80 | train_loss=0.6755 acc=72.14% | val_loss=0.8192 acc@1=83.36% | time=460.7s\n",
      "  Epoch 050/80 | train_loss=0.6657 acc=69.44% | val_loss=0.8558 acc@1=82.72% | time=400.5s\n",
      "  Epoch 051/80 | train_loss=0.6470 acc=70.76% | val_loss=0.8762 acc@1=83.70% | time=388.8s\n",
      "  Epoch 052/80 | train_loss=0.6330 acc=70.66% | val_loss=0.8716 acc@1=83.64% | time=387.8s\n",
      "  Epoch 053/80 | train_loss=0.6287 acc=70.10% | val_loss=0.8586 acc@1=83.66% | time=386.6s\n",
      "  Epoch 054/80 | train_loss=0.6210 acc=69.87% | val_loss=0.9123 acc@1=82.88% | time=358.5s\n",
      "  Epoch 055/80 | train_loss=0.6324 acc=70.16% | val_loss=0.8374 acc@1=84.20% | time=301.9s\n",
      "  Epoch 056/80 | train_loss=0.6336 acc=70.97% | val_loss=0.8558 acc@1=83.42% | time=286.0s\n",
      "  Epoch 057/80 | train_loss=0.6359 acc=69.63% | val_loss=0.8469 acc@1=83.80% | time=278.8s\n",
      "  Epoch 058/80 | train_loss=0.6041 acc=69.77% | val_loss=0.8964 acc@1=83.62% | time=282.3s\n",
      "  Epoch 059/80 | train_loss=0.5994 acc=71.91% | val_loss=0.8641 acc@1=83.66% | time=282.0s\n",
      "  Epoch 060/80 | train_loss=0.6221 acc=70.10% | val_loss=0.8283 acc@1=84.32% | time=281.0s\n",
      "  Epoch 061/80 | train_loss=0.6156 acc=70.53% | val_loss=0.8028 acc@1=84.60% | time=278.9s\n",
      "  Epoch 062/80 | train_loss=0.5949 acc=72.42% | val_loss=0.8488 acc@1=84.12% | time=280.6s\n",
      "  Epoch 063/80 | train_loss=0.6117 acc=69.25% | val_loss=0.8392 acc@1=84.84% | time=279.6s\n",
      "  Epoch 064/80 | train_loss=0.5812 acc=69.62% | val_loss=0.8774 acc@1=84.14% | time=279.8s\n",
      "  Epoch 065/80 | train_loss=0.6178 acc=69.72% | val_loss=0.8941 acc@1=83.94% | time=280.8s\n",
      "  Epoch 066/80 | train_loss=0.5524 acc=71.22% | val_loss=0.8546 acc@1=84.32% | time=280.0s\n",
      "  Epoch 067/80 | train_loss=0.6005 acc=69.84% | val_loss=0.8397 acc@1=84.50% | time=278.9s\n",
      "  Epoch 068/80 | train_loss=0.5822 acc=70.16% | val_loss=0.8292 acc@1=84.38% | time=276.5s\n",
      "  Epoch 069/80 | train_loss=0.5999 acc=69.94% | val_loss=0.8687 acc@1=84.48% | time=277.9s\n",
      "  Epoch 070/80 | train_loss=0.5725 acc=71.03% | val_loss=0.8309 acc@1=84.94% | time=281.4s\n",
      "  Epoch 071/80 | train_loss=0.5539 acc=70.69% | val_loss=0.8570 acc@1=84.84% | time=281.2s\n",
      "  Epoch 072/80 | train_loss=0.5734 acc=71.34% | val_loss=0.8348 acc@1=84.56% | time=278.2s\n",
      "  Epoch 073/80 | train_loss=0.5640 acc=72.36% | val_loss=0.8513 acc@1=84.76% | time=279.8s\n",
      "  Epoch 074/80 | train_loss=0.5438 acc=73.04% | val_loss=0.8506 acc@1=85.02% | time=278.4s\n",
      "  Epoch 075/80 | train_loss=0.5474 acc=71.55% | val_loss=0.8588 acc@1=84.44% | time=279.6s\n",
      "  Epoch 076/80 | train_loss=0.5584 acc=71.49% | val_loss=0.8542 acc@1=84.58% | time=275.1s\n",
      "  Epoch 077/80 | train_loss=0.5297 acc=72.35% | val_loss=0.8442 acc@1=84.58% | time=277.2s\n",
      "  Epoch 078/80 | train_loss=0.5772 acc=71.17% | val_loss=0.8611 acc@1=84.30% | time=277.5s\n",
      "  Epoch 079/80 | train_loss=0.5649 acc=71.05% | val_loss=0.8517 acc@1=85.04% | time=277.4s\n",
      "  Epoch 080/80 | train_loss=0.5614 acc=71.30% | val_loss=0.8408 acc@1=84.84% | time=275.4s\n",
      "  -> Best checkpoint: ./outputs\\convnext_tiny_e80_bs32_acc2_20251202-170955\\checkpoints\\convnext_tiny_best.pt\n",
      "  -> Plots: ./outputs\\convnext_tiny_e80_bs32_acc2_20251202-170955\\plots\n",
      "  -> Results: ./outputs\\convnext_tiny_e80_bs32_acc2_20251202-170955\\results\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'convnext_tiny',\n",
       " 'epochs_run': 80,\n",
       " 'optimizer': 'adamw',\n",
       " 'lr_final': 0.0,\n",
       " 'weight_decay': 0.05,\n",
       " 'batch_size': 32,\n",
       " 'accum_steps': 2,\n",
       " 'max_grad_norm': 1.0,\n",
       " 'label_smoothing': 0.0,\n",
       " 'mixup_cutmix': {'enabled': True,\n",
       "  'prob': 0.8,\n",
       "  'mixup_alpha': 0.2,\n",
       "  'cutmix_alpha': 1.0,\n",
       "  'switch_prob': 0.5},\n",
       " 'ema': {'enabled': True, 'decay': 0.9999, 'eval': False},\n",
       " 'val': {'loss': 0.8627401709794998, 'top1': 0.8468, 'top5': 0.9638},\n",
       " 'test': {'loss': 0.6566876657158136, 'top1': 0.8788, 'top5': 0.9755},\n",
       " 'ece_raw': 0.08491486203968526,\n",
       " 'ece_temp_scaled': 0.018031411035358904,\n",
       " 'temperature': 1.5065102577209473,\n",
       " 'checkpoint': './outputs\\\\convnext_tiny_e80_bs32_acc2_20251202-170955\\\\checkpoints\\\\convnext_tiny_best.pt',\n",
       " 'run_dir': './outputs\\\\convnext_tiny_e80_bs32_acc2_20251202-170955'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convnext_metrics = train_and_evaluate_one_model('convnext_tiny')\n",
    "convnext_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232b99f5",
   "metadata": {},
   "source": [
    "## 14) Train: ViT‑Tiny (DeiT‑Tiny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2551c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "==> [vit_tiny] Training for 100 epochs | opt=adamw lr=0.0005 wd=0.05 | img=224 | bs=32 | accum=2\n",
      "  Epoch 001/100 | train_loss=3.9940 acc=11.44% | val_loss=1.8894 acc@1=49.22% | time=106.4s\n",
      "  Epoch 002/100 | train_loss=2.5405 acc=33.56% | val_loss=1.5944 acc@1=56.82% | time=106.9s\n",
      "  Epoch 003/100 | train_loss=2.4715 acc=34.81% | val_loss=1.6897 acc@1=55.36% | time=106.2s\n",
      "  Epoch 004/100 | train_loss=2.3813 acc=35.92% | val_loss=1.5153 acc@1=58.56% | time=107.3s\n",
      "  Epoch 005/100 | train_loss=2.2971 acc=37.54% | val_loss=1.4419 acc@1=60.46% | time=106.9s\n",
      "  Epoch 006/100 | train_loss=2.2002 acc=39.33% | val_loss=1.3748 acc@1=61.86% | time=107.2s\n",
      "  Epoch 007/100 | train_loss=2.1940 acc=40.62% | val_loss=1.3898 acc@1=61.28% | time=107.7s\n",
      "  Epoch 008/100 | train_loss=2.0829 acc=41.63% | val_loss=1.3359 acc@1=62.96% | time=106.7s\n",
      "  Epoch 009/100 | train_loss=2.0265 acc=42.78% | val_loss=1.2797 acc@1=65.26% | time=107.3s\n",
      "  Epoch 010/100 | train_loss=1.9930 acc=43.88% | val_loss=1.2815 acc@1=64.24% | time=105.8s\n",
      "  Epoch 011/100 | train_loss=1.9425 acc=44.35% | val_loss=1.2526 acc@1=65.90% | time=105.5s\n",
      "  Epoch 012/100 | train_loss=1.9172 acc=44.78% | val_loss=1.2374 acc@1=64.72% | time=107.6s\n",
      "  Epoch 013/100 | train_loss=1.8934 acc=45.04% | val_loss=1.2089 acc@1=66.42% | time=106.6s\n",
      "  Epoch 014/100 | train_loss=1.8698 acc=45.82% | val_loss=1.2184 acc@1=66.58% | time=105.6s\n",
      "  Epoch 015/100 | train_loss=1.8511 acc=47.38% | val_loss=1.1468 acc@1=67.82% | time=105.6s\n",
      "  Epoch 016/100 | train_loss=1.7805 acc=48.56% | val_loss=1.1780 acc@1=66.50% | time=107.0s\n",
      "  Epoch 017/100 | train_loss=1.7409 acc=47.72% | val_loss=1.1700 acc@1=67.86% | time=106.5s\n",
      "  Epoch 018/100 | train_loss=1.7853 acc=48.48% | val_loss=1.1304 acc@1=68.26% | time=107.0s\n",
      "  Epoch 019/100 | train_loss=1.7420 acc=49.21% | val_loss=1.1134 acc@1=68.90% | time=106.7s\n",
      "  Epoch 020/100 | train_loss=1.6828 acc=50.75% | val_loss=1.0880 acc@1=69.52% | time=107.7s\n",
      "  Epoch 021/100 | train_loss=1.7057 acc=50.90% | val_loss=1.0900 acc@1=69.46% | time=106.8s\n",
      "  Epoch 022/100 | train_loss=1.6614 acc=49.97% | val_loss=1.1177 acc@1=69.14% | time=106.7s\n",
      "  Epoch 023/100 | train_loss=1.7038 acc=49.85% | val_loss=1.1303 acc@1=68.46% | time=106.5s\n",
      "  Epoch 024/100 | train_loss=1.5989 acc=51.71% | val_loss=1.0991 acc@1=70.26% | time=105.2s\n",
      "  Epoch 025/100 | train_loss=1.6016 acc=51.61% | val_loss=1.1107 acc@1=70.44% | time=105.6s\n",
      "  Epoch 026/100 | train_loss=1.6386 acc=51.41% | val_loss=1.0718 acc@1=70.60% | time=106.5s\n",
      "  Epoch 027/100 | train_loss=1.5849 acc=52.34% | val_loss=1.0736 acc@1=70.14% | time=106.7s\n",
      "  Epoch 028/100 | train_loss=1.5440 acc=52.77% | val_loss=1.0229 acc@1=71.12% | time=105.9s\n",
      "  Epoch 029/100 | train_loss=1.5741 acc=52.30% | val_loss=1.0301 acc@1=71.22% | time=106.5s\n",
      "  Epoch 030/100 | train_loss=1.5172 acc=52.12% | val_loss=1.0320 acc@1=71.18% | time=107.0s\n",
      "  Epoch 031/100 | train_loss=1.5036 acc=55.60% | val_loss=1.0696 acc@1=71.00% | time=106.5s\n",
      "  Epoch 032/100 | train_loss=1.5226 acc=53.48% | val_loss=1.0042 acc@1=71.96% | time=107.6s\n",
      "  Epoch 033/100 | train_loss=1.4792 acc=53.94% | val_loss=1.0170 acc@1=72.50% | time=107.3s\n",
      "  Epoch 034/100 | train_loss=1.4727 acc=53.73% | val_loss=1.0164 acc@1=72.08% | time=106.4s\n",
      "  Epoch 035/100 | train_loss=1.4083 acc=55.03% | val_loss=1.0935 acc@1=70.96% | time=107.0s\n",
      "  Epoch 036/100 | train_loss=1.4562 acc=54.59% | val_loss=1.0234 acc@1=72.04% | time=106.6s\n",
      "  Epoch 037/100 | train_loss=1.4548 acc=54.95% | val_loss=0.9909 acc@1=72.80% | time=106.1s\n",
      "  Epoch 038/100 | train_loss=1.3798 acc=57.21% | val_loss=1.0250 acc@1=72.26% | time=107.3s\n",
      "  Epoch 039/100 | train_loss=1.3866 acc=56.09% | val_loss=1.0099 acc@1=72.76% | time=106.3s\n",
      "  Epoch 040/100 | train_loss=1.3638 acc=55.97% | val_loss=1.0785 acc@1=71.46% | time=105.7s\n",
      "  Epoch 041/100 | train_loss=1.3842 acc=57.78% | val_loss=1.0229 acc@1=73.12% | time=107.6s\n",
      "  Epoch 042/100 | train_loss=1.2866 acc=58.03% | val_loss=1.0132 acc@1=72.58% | time=108.3s\n",
      "  Epoch 043/100 | train_loss=1.3503 acc=59.85% | val_loss=1.0151 acc@1=73.16% | time=106.9s\n",
      "  Epoch 044/100 | train_loss=1.3194 acc=58.48% | val_loss=0.9787 acc@1=73.60% | time=105.7s\n",
      "  Epoch 045/100 | train_loss=1.3206 acc=58.01% | val_loss=1.0089 acc@1=73.48% | time=106.1s\n",
      "  Epoch 046/100 | train_loss=1.2545 acc=60.13% | val_loss=0.9843 acc@1=73.76% | time=106.0s\n",
      "  Epoch 047/100 | train_loss=1.3082 acc=57.50% | val_loss=0.9593 acc@1=75.20% | time=106.3s\n",
      "  Epoch 048/100 | train_loss=1.2390 acc=59.83% | val_loss=0.9536 acc@1=75.46% | time=106.4s\n",
      "  Epoch 049/100 | train_loss=1.2564 acc=58.80% | val_loss=0.9493 acc@1=74.62% | time=106.5s\n",
      "  Epoch 050/100 | train_loss=1.1964 acc=61.39% | val_loss=0.9632 acc@1=74.34% | time=105.8s\n",
      "  Epoch 051/100 | train_loss=1.2193 acc=60.99% | val_loss=0.9958 acc@1=73.82% | time=105.9s\n",
      "  Epoch 052/100 | train_loss=1.2124 acc=60.99% | val_loss=0.9756 acc@1=75.26% | time=105.4s\n",
      "  Epoch 053/100 | train_loss=1.2279 acc=59.02% | val_loss=0.9012 acc@1=76.42% | time=107.4s\n",
      "  Epoch 054/100 | train_loss=1.1987 acc=61.59% | val_loss=0.9452 acc@1=75.38% | time=106.3s\n",
      "  Epoch 055/100 | train_loss=1.1756 acc=61.52% | val_loss=0.9421 acc@1=75.34% | time=107.5s\n",
      "  Epoch 056/100 | train_loss=1.1406 acc=61.35% | val_loss=0.9440 acc@1=75.52% | time=107.3s\n",
      "  Epoch 057/100 | train_loss=1.1267 acc=61.34% | val_loss=0.9174 acc@1=76.74% | time=106.6s\n",
      "  Epoch 058/100 | train_loss=1.1145 acc=62.30% | val_loss=0.9503 acc@1=76.48% | time=106.9s\n",
      "  Epoch 059/100 | train_loss=1.1015 acc=62.47% | val_loss=0.9043 acc@1=77.08% | time=107.4s\n",
      "  Epoch 060/100 | train_loss=1.1308 acc=63.24% | val_loss=0.9388 acc@1=75.46% | time=106.3s\n",
      "  Epoch 061/100 | train_loss=1.0980 acc=60.74% | val_loss=0.9270 acc@1=76.78% | time=105.6s\n",
      "  Epoch 062/100 | train_loss=1.1170 acc=62.19% | val_loss=0.9261 acc@1=77.48% | time=107.5s\n",
      "  Epoch 063/100 | train_loss=1.1092 acc=62.86% | val_loss=0.9199 acc@1=75.74% | time=106.5s\n",
      "  Epoch 064/100 | train_loss=1.0596 acc=63.11% | val_loss=0.9106 acc@1=77.26% | time=106.6s\n",
      "  Epoch 065/100 | train_loss=1.0542 acc=62.45% | val_loss=0.8794 acc@1=77.90% | time=106.8s\n",
      "  Epoch 066/100 | train_loss=1.1123 acc=63.88% | val_loss=0.9104 acc@1=77.34% | time=108.5s\n",
      "  Epoch 067/100 | train_loss=1.0655 acc=64.32% | val_loss=0.9017 acc@1=77.94% | time=106.7s\n",
      "  Epoch 068/100 | train_loss=1.0059 acc=66.03% | val_loss=0.9019 acc@1=77.50% | time=106.6s\n",
      "  Epoch 069/100 | train_loss=1.0114 acc=64.94% | val_loss=0.8877 acc@1=77.52% | time=106.4s\n",
      "  Epoch 070/100 | train_loss=1.0093 acc=63.06% | val_loss=0.9113 acc@1=77.50% | time=107.1s\n",
      "  Epoch 071/100 | train_loss=0.9830 acc=65.58% | val_loss=0.9056 acc@1=77.46% | time=106.0s\n",
      "  Epoch 072/100 | train_loss=0.9650 acc=64.78% | val_loss=0.8902 acc@1=78.02% | time=107.1s\n",
      "  Epoch 073/100 | train_loss=0.9677 acc=65.08% | val_loss=0.8825 acc@1=78.84% | time=106.1s\n",
      "  Epoch 074/100 | train_loss=0.9611 acc=66.44% | val_loss=0.8662 acc@1=78.46% | time=106.8s\n",
      "  Epoch 075/100 | train_loss=1.0209 acc=64.54% | val_loss=0.8820 acc@1=78.30% | time=106.0s\n",
      "  Epoch 076/100 | train_loss=0.9802 acc=65.77% | val_loss=0.8648 acc@1=78.46% | time=106.6s\n",
      "  Epoch 077/100 | train_loss=0.9419 acc=66.81% | val_loss=0.8728 acc@1=78.92% | time=105.5s\n",
      "  Epoch 078/100 | train_loss=0.9577 acc=67.21% | val_loss=0.8563 acc@1=79.44% | time=105.8s\n",
      "  Epoch 079/100 | train_loss=0.9300 acc=64.81% | val_loss=0.8550 acc@1=79.42% | time=105.9s\n",
      "  Epoch 080/100 | train_loss=0.9592 acc=64.05% | val_loss=0.8642 acc@1=79.36% | time=106.3s\n",
      "  Epoch 081/100 | train_loss=0.8882 acc=66.51% | val_loss=0.8323 acc@1=79.64% | time=106.6s\n",
      "  Epoch 082/100 | train_loss=0.9277 acc=66.17% | val_loss=0.8415 acc@1=79.38% | time=108.0s\n",
      "  Epoch 083/100 | train_loss=0.9497 acc=65.33% | val_loss=0.8665 acc@1=79.70% | time=106.8s\n",
      "  Epoch 084/100 | train_loss=0.9045 acc=64.75% | val_loss=0.8396 acc@1=80.10% | time=107.7s\n",
      "  Epoch 085/100 | train_loss=0.9019 acc=67.84% | val_loss=0.8577 acc@1=79.80% | time=106.7s\n",
      "  Epoch 086/100 | train_loss=0.8912 acc=67.00% | val_loss=0.8495 acc@1=80.12% | time=107.4s\n",
      "  Epoch 087/100 | train_loss=0.9032 acc=67.70% | val_loss=0.8340 acc@1=80.30% | time=106.4s\n",
      "  Epoch 088/100 | train_loss=0.8641 acc=67.45% | val_loss=0.8526 acc@1=79.96% | time=105.0s\n",
      "  Epoch 089/100 | train_loss=0.8151 acc=66.57% | val_loss=0.8345 acc@1=80.24% | time=105.9s\n",
      "  Epoch 090/100 | train_loss=0.8858 acc=67.02% | val_loss=0.8243 acc@1=80.62% | time=107.3s\n",
      "  Epoch 091/100 | train_loss=0.8951 acc=65.68% | val_loss=0.8481 acc@1=80.66% | time=107.5s\n",
      "  Epoch 092/100 | train_loss=0.8842 acc=66.30% | val_loss=0.7850 acc@1=80.98% | time=106.2s\n",
      "  Epoch 093/100 | train_loss=0.8677 acc=66.07% | val_loss=0.8158 acc@1=80.84% | time=106.2s\n",
      "  Epoch 094/100 | train_loss=0.8737 acc=66.53% | val_loss=0.8293 acc@1=80.38% | time=106.5s\n",
      "  Epoch 095/100 | train_loss=0.8788 acc=67.03% | val_loss=0.8300 acc@1=80.46% | time=106.8s\n",
      "  Epoch 096/100 | train_loss=0.8613 acc=66.97% | val_loss=0.8264 acc@1=80.24% | time=108.1s\n",
      "  Epoch 097/100 | train_loss=0.8755 acc=66.41% | val_loss=0.8195 acc@1=81.18% | time=104.4s\n",
      "  Epoch 098/100 | train_loss=0.8778 acc=67.69% | val_loss=0.8040 acc@1=80.86% | time=106.4s\n",
      "  Epoch 099/100 | train_loss=0.8598 acc=68.98% | val_loss=0.8394 acc@1=80.18% | time=106.4s\n",
      "  Epoch 100/100 | train_loss=0.8560 acc=68.00% | val_loss=0.7967 acc@1=80.92% | time=105.6s\n",
      "  -> Best checkpoint: ./outputs\\vit_tiny_e100_bs32_acc2_20251203-023446\\checkpoints\\vit_tiny_best.pt\n",
      "  -> Plots: ./outputs\\vit_tiny_e100_bs32_acc2_20251203-023446\\plots\n",
      "  -> Results: ./outputs\\vit_tiny_e100_bs32_acc2_20251203-023446\\results\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'vit_tiny',\n",
       " 'epochs_run': 100,\n",
       " 'optimizer': 'adamw',\n",
       " 'lr_final': 0.0,\n",
       " 'weight_decay': 0.05,\n",
       " 'batch_size': 32,\n",
       " 'accum_steps': 2,\n",
       " 'max_grad_norm': 1.0,\n",
       " 'label_smoothing': 0.0,\n",
       " 'mixup_cutmix': {'enabled': True,\n",
       "  'prob': 0.8,\n",
       "  'mixup_alpha': 0.2,\n",
       "  'cutmix_alpha': 1.0,\n",
       "  'switch_prob': 0.5},\n",
       " 'ema': {'enabled': True, 'decay': 0.9999, 'eval': False},\n",
       " 'val': {'loss': 0.7983950404942036, 'top1': 0.8124, 'top5': 0.9526},\n",
       " 'test': {'loss': 0.671459115076065, 'top1': 0.841, 'top5': 0.9637},\n",
       " 'ece_raw': 0.057714962327480315,\n",
       " 'ece_temp_scaled': 0.012789853978157045,\n",
       " 'temperature': 1.1999342441558838,\n",
       " 'checkpoint': './outputs\\\\vit_tiny_e100_bs32_acc2_20251203-023446\\\\checkpoints\\\\vit_tiny_best.pt',\n",
       " 'run_dir': './outputs\\\\vit_tiny_e100_bs32_acc2_20251203-023446'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_tiny_metrics = train_and_evaluate_one_model('vit_tiny')\n",
    "vit_tiny_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c9f762",
   "metadata": {},
   "source": [
    "## 15) Train: ViT‑Hybrid (ResNet‑26 + ViT Small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14f7b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "==> [vit_hybrid] Training for 100 epochs | opt=adamw lr=0.0005 wd=0.05 | img=224 | bs=32 | accum=2\n",
      "  Epoch 001/100 | train_loss=3.0226 acc=28.98% | val_loss=1.0108 acc@1=72.40% | time=383.1s\n",
      "  Epoch 002/100 | train_loss=1.9644 acc=45.95% | val_loss=1.0977 acc@1=70.56% | time=378.8s\n",
      "  Epoch 003/100 | train_loss=2.0293 acc=42.95% | val_loss=1.3751 acc@1=62.84% | time=378.4s\n",
      "  Epoch 004/100 | train_loss=2.0624 acc=42.37% | val_loss=1.3398 acc@1=63.32% | time=378.1s\n",
      "  Epoch 005/100 | train_loss=2.0233 acc=43.31% | val_loss=1.2118 acc@1=66.30% | time=377.7s\n",
      "  Epoch 006/100 | train_loss=1.9054 acc=45.73% | val_loss=1.1450 acc@1=68.80% | time=379.3s\n",
      "  Epoch 007/100 | train_loss=1.8399 acc=45.09% | val_loss=1.1074 acc@1=69.40% | time=378.3s\n",
      "  Epoch 008/100 | train_loss=1.7718 acc=48.84% | val_loss=1.1348 acc@1=68.68% | time=379.4s\n",
      "  Epoch 009/100 | train_loss=1.7474 acc=47.61% | val_loss=1.1220 acc@1=68.82% | time=378.8s\n",
      "  Epoch 010/100 | train_loss=1.6651 acc=48.55% | val_loss=1.0626 acc@1=70.48% | time=379.0s\n",
      "  Epoch 011/100 | train_loss=1.6812 acc=49.66% | val_loss=1.0104 acc@1=71.70% | time=377.6s\n",
      "  Epoch 012/100 | train_loss=1.6223 acc=50.47% | val_loss=1.0321 acc@1=70.98% | time=378.7s\n",
      "  Epoch 013/100 | train_loss=1.6141 acc=51.62% | val_loss=1.0190 acc@1=72.02% | time=378.6s\n",
      "  Epoch 014/100 | train_loss=1.5708 acc=51.30% | val_loss=0.9941 acc@1=72.08% | time=378.6s\n",
      "  Epoch 015/100 | train_loss=1.5631 acc=53.46% | val_loss=1.0269 acc@1=71.78% | time=377.5s\n",
      "  Epoch 016/100 | train_loss=1.5127 acc=54.52% | val_loss=0.9728 acc@1=72.46% | time=379.3s\n",
      "  Epoch 017/100 | train_loss=1.4765 acc=55.74% | val_loss=0.9742 acc@1=73.46% | time=378.2s\n",
      "  Epoch 018/100 | train_loss=1.4724 acc=54.10% | val_loss=0.9566 acc@1=74.10% | time=377.6s\n",
      "  Epoch 019/100 | train_loss=1.3888 acc=56.04% | val_loss=0.8985 acc@1=74.80% | time=379.3s\n",
      "  Epoch 020/100 | train_loss=1.3658 acc=55.94% | val_loss=0.9399 acc@1=74.38% | time=379.4s\n",
      "  Epoch 021/100 | train_loss=1.3549 acc=57.45% | val_loss=0.9973 acc@1=72.70% | time=376.7s\n",
      "  Epoch 022/100 | train_loss=1.3740 acc=56.81% | val_loss=0.9256 acc@1=74.90% | time=375.9s\n",
      "  Epoch 023/100 | train_loss=1.3839 acc=54.82% | val_loss=0.9513 acc@1=73.52% | time=377.1s\n",
      "  Epoch 024/100 | train_loss=1.2967 acc=57.86% | val_loss=0.9697 acc@1=74.44% | time=377.1s\n",
      "  Epoch 025/100 | train_loss=1.3246 acc=58.94% | val_loss=0.9325 acc@1=75.20% | time=375.6s\n",
      "  Epoch 026/100 | train_loss=1.3127 acc=57.32% | val_loss=0.9232 acc@1=74.84% | time=377.9s\n",
      "  Epoch 027/100 | train_loss=1.2598 acc=57.63% | val_loss=0.8999 acc@1=75.86% | time=378.8s\n",
      "  Epoch 028/100 | train_loss=1.2244 acc=59.32% | val_loss=0.9257 acc@1=76.06% | time=378.4s\n",
      "  Epoch 029/100 | train_loss=1.2140 acc=61.26% | val_loss=0.9549 acc@1=75.00% | time=378.1s\n",
      "  Epoch 030/100 | train_loss=1.1952 acc=59.76% | val_loss=0.9431 acc@1=75.36% | time=445.5s\n",
      "  Epoch 031/100 | train_loss=1.1991 acc=59.42% | val_loss=0.9364 acc@1=75.80% | time=527.0s\n",
      "  Epoch 032/100 | train_loss=1.2025 acc=58.72% | val_loss=0.9022 acc@1=76.12% | time=474.7s\n",
      "  Epoch 033/100 | train_loss=1.1397 acc=60.98% | val_loss=0.9207 acc@1=76.14% | time=414.7s\n",
      "  Epoch 034/100 | train_loss=1.1324 acc=60.52% | val_loss=0.9278 acc@1=76.06% | time=659.9s\n",
      "  Epoch 035/100 | train_loss=1.1309 acc=62.28% | val_loss=0.9063 acc@1=76.68% | time=651.2s\n",
      "  Epoch 036/100 | train_loss=1.1118 acc=60.09% | val_loss=0.9563 acc@1=76.12% | time=617.5s\n",
      "  Epoch 037/100 | train_loss=1.1296 acc=61.65% | val_loss=0.9747 acc@1=75.92% | time=646.7s\n",
      "  Epoch 038/100 | train_loss=1.0788 acc=61.86% | val_loss=0.9114 acc@1=76.86% | time=442.3s\n",
      "  Epoch 039/100 | train_loss=1.1119 acc=63.90% | val_loss=0.9405 acc@1=76.00% | time=449.9s\n",
      "  Epoch 040/100 | train_loss=1.0758 acc=63.46% | val_loss=0.9067 acc@1=76.80% | time=396.4s\n",
      "  Epoch 041/100 | train_loss=1.0544 acc=63.28% | val_loss=0.9601 acc@1=76.18% | time=442.6s\n",
      "  Epoch 042/100 | train_loss=1.0256 acc=64.35% | val_loss=0.9436 acc@1=77.26% | time=634.6s\n",
      "  Epoch 043/100 | train_loss=1.0537 acc=63.87% | val_loss=0.9233 acc@1=76.84% | time=656.5s\n",
      "  Epoch 044/100 | train_loss=1.0260 acc=63.54% | val_loss=0.9016 acc@1=77.92% | time=594.7s\n",
      "  Epoch 045/100 | train_loss=1.0083 acc=64.46% | val_loss=0.9091 acc@1=77.78% | time=551.7s\n",
      "  Epoch 046/100 | train_loss=1.0199 acc=64.35% | val_loss=0.9450 acc@1=76.62% | time=427.6s\n",
      "  Epoch 047/100 | train_loss=0.9843 acc=64.88% | val_loss=0.9078 acc@1=77.76% | time=656.6s\n",
      "  Epoch 048/100 | train_loss=1.0085 acc=64.71% | val_loss=0.9066 acc@1=77.20% | time=653.7s\n",
      "  Epoch 049/100 | train_loss=0.9850 acc=64.49% | val_loss=0.8874 acc@1=78.40% | time=600.7s\n",
      "  Epoch 050/100 | train_loss=0.9548 acc=63.71% | val_loss=0.9120 acc@1=77.78% | time=637.5s\n",
      "  Epoch 051/100 | train_loss=0.9396 acc=64.86% | val_loss=0.9473 acc@1=78.22% | time=745.6s\n",
      "  Epoch 052/100 | train_loss=0.9179 acc=67.93% | val_loss=0.9056 acc@1=78.58% | time=676.1s\n",
      "  Epoch 053/100 | train_loss=0.9107 acc=65.51% | val_loss=0.9402 acc@1=77.80% | time=937.0s\n",
      "  Epoch 054/100 | train_loss=0.9068 acc=66.29% | val_loss=0.8458 acc@1=78.92% | time=801.8s\n",
      "  Epoch 055/100 | train_loss=0.9109 acc=66.11% | val_loss=0.9313 acc@1=79.08% | time=844.0s\n",
      "  Epoch 056/100 | train_loss=0.8986 acc=68.79% | val_loss=0.8560 acc@1=79.06% | time=712.7s\n",
      "  Epoch 057/100 | train_loss=0.8539 acc=68.16% | val_loss=0.9293 acc@1=79.40% | time=737.8s\n",
      "  Epoch 058/100 | train_loss=0.8601 acc=66.84% | val_loss=0.9373 acc@1=78.04% | time=669.2s\n",
      "  Epoch 059/100 | train_loss=0.8754 acc=67.07% | val_loss=0.8538 acc@1=79.44% | time=530.8s\n",
      "  Epoch 060/100 | train_loss=0.8253 acc=68.06% | val_loss=0.9546 acc@1=79.26% | time=520.1s\n",
      "  Epoch 061/100 | train_loss=0.8137 acc=66.63% | val_loss=0.8985 acc@1=80.40% | time=516.8s\n",
      "  Epoch 062/100 | train_loss=0.8147 acc=66.65% | val_loss=0.9018 acc@1=79.62% | time=702.1s\n",
      "  Epoch 063/100 | train_loss=0.8131 acc=67.91% | val_loss=0.8906 acc@1=80.16% | time=654.0s\n",
      "  Epoch 064/100 | train_loss=0.8238 acc=65.49% | val_loss=0.9579 acc@1=80.24% | time=666.7s\n"
     ]
    }
   ],
   "source": [
    "vit_hybrid_metrics = train_and_evaluate_one_model('vit_hybrid')\n",
    "vit_hybrid_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b319f6b6",
   "metadata": {},
   "source": [
    "## 16) Aggregate results across all runs (auto‑discover from ./outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c98c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def discover_runs_and_build_df(outputs_root='./outputs'):\n",
    "    rows = []\n",
    "    for root, dirs, files in os.walk(outputs_root):\n",
    "        if 'metrics.json' in files:\n",
    "            path = os.path.join(root, 'metrics.json')\n",
    "            try:\n",
    "                with open(path, 'r') as f:\n",
    "                    m = json.load(f)\n",
    "                rows.append({\n",
    "                    'model': m['model'], 'val_top1': m['val']['top1'], 'val_top5': m['val']['top5'],\n",
    "                    'test_top1': m['test']['top1'], 'test_top5': m['test']['top5'],\n",
    "                    'ece_raw': m.get('ece_raw', None), 'ece_temp': m.get('ece_temp_scaled', None),\n",
    "                    'temperature': m.get('temperature', None),\n",
    "                    'checkpoint': m['checkpoint'], 'run_dir': m['run_dir']\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(\"Skip\", path, e)\n",
    "    if len(rows) == 0:\n",
    "        print(\"No metrics.json found yet.\"); return None\n",
    "    df = pd.DataFrame(rows).sort_values(['model','test_top1'], ascending=[True, False]).groupby('model', as_index=False).first()\n",
    "    SUMMARY_DIR = os.path.join(outputs_root, 'summary'); ensure_dir(SUMMARY_DIR)\n",
    "    df.to_csv(os.path.join(SUMMARY_DIR, 'aggregate_results.csv'), index=False)\n",
    "    print(\"Aggregate saved to:\", os.path.join(SUMMARY_DIR, 'aggregate_results.csv'))\n",
    "    return df\n",
    "\n",
    "df = discover_runs_and_build_df('./outputs')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf0e137",
   "metadata": {},
   "source": [
    "## 17) Summary Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a33d5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    SUMMARY_DIR = os.path.join(\"./outputs\", \"summary\"); ensure_dir(SUMMARY_DIR)\n",
    "    csv_path = os.path.join(SUMMARY_DIR, \"aggregate_results.csv\")\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        plt.figure(figsize=(7,5)); plt.bar(df[\"model\"], df[\"test_top1\"])\n",
    "        plt.ylabel(\"Test Top-1 Accuracy\"); plt.title(\"Model Comparison: Top-1\")\n",
    "        plt.grid(True, axis=\"y\", alpha=0.3); plt.tight_layout()\n",
    "        plt.savefig(os.path.join(SUMMARY_DIR, \"summary_top1.png\"), dpi=150); plt.close()\n",
    "\n",
    "        plt.figure(figsize=(7,5)); plt.bar(df[\"model\"], df[\"test_top5\"])\n",
    "        plt.ylabel(\"Test Top-5 Accuracy\"); plt.title(\"Model Comparison: Top-5\")\n",
    "        plt.grid(True, axis=\"y\", alpha=0.3); plt.tight_layout()\n",
    "        plt.savefig(os.path.join(SUMMARY_DIR, \"summary_top5.png\"), dpi=150); plt.close()\n",
    "\n",
    "        if \"ece_raw\" in df.columns and df[\"ece_raw\"].notnull().any():\n",
    "            plt.figure(figsize=(7,5)); plt.bar(df[\"model\"], df[\"ece_raw\"])\n",
    "            plt.ylabel(\"ECE (raw)\"); plt.title(\"Calibration (ECE raw)\")\n",
    "            plt.grid(True, axis=\"y\", alpha=0.3); plt.tight_layout()\n",
    "            plt.savefig(os.path.join(SUMMARY_DIR, \"summary_ece_raw.png\"), dpi=150); plt.close()\n",
    "\n",
    "        if \"ece_temp\" in df.columns and df[\"ece_temp\"].notnull().any():\n",
    "            plt.figure(figsize=(7,5)); plt.bar(df[\"model\"], df[\"ece_temp\"])\n",
    "            plt.ylabel(\"ECE (temp‑scaled)\"); plt.title(\"Calibration (ECE after Temp Scaling)\")\n",
    "            plt.grid(True, axis=\"y\", alpha=0.3); plt.tight_layout()\n",
    "            plt.savefig(os.path.join(SUMMARY_DIR, \"summary_ece_temp.png\"), dpi=150); plt.close()\n",
    "\n",
    "        print(\"Saved summary plots in:\", SUMMARY_DIR)\n",
    "    else:\n",
    "        print(\"No aggregate_results.csv found; run the aggregation cell above first.\")\n",
    "except Exception as e:\n",
    "    print(\"Summary plot error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b87579",
   "metadata": {},
   "source": [
    "## 18) One‑click ZIP export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a03689",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "zip_name = f\"cifar100_capstone_outputs_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "archive_path = shutil.make_archive(zip_name, \"zip\", \"./outputs\")\n",
    "print(\"Created ZIP:\", os.path.abspath(archive_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d606e0a",
   "metadata": {},
   "source": [
    "\n",
    "## 19) How to run & tips\n",
    "\n",
    "- **GPU check**: The notebook asserts CUDA by default. Switch `REQUIRE_GPU=False` if you want CPU (very slow).\n",
    "- **If you see CUDA OOM**: Lower `MODEL_CFG[model]['batch']` and increase `accum` accordingly.\n",
    "- **Early stop targets** are preset (e.g., 0.78 for WRN/ConvNeXt). Adjust in `MODEL_CFG` or set to `None`.\n",
    "- **TensorBoard**: logs in `./outputs/<run>/tb`. Launch: `tensorboard --logdir outputs`\n",
    "- **W&B**: set `LOG_WANDB=True`, `pip install wandb`, `wandb login`, (optional) set `WANDB_ENTITY`.\n",
    "- **Artifacts per model**: best checkpoint, plots, metrics.json, history.csv, confusion_matrix.npy, reliability bins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee7d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- One-batch sanity check for validation (optional) ---\n",
    "try:\n",
    "    _m = model; _lv = dl_val\n",
    "    _m.eval()\n",
    "    _ce = torch.nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        x, y = next(iter(_lv))\n",
    "        x = x.to(DEVICE); y = y.to(DEVICE).long()\n",
    "        with torch.amp.autocast('cuda', enabled=False):\n",
    "            logits = _m(x).float()\n",
    "            loss = _ce(logits, y)\n",
    "        acc1 = (logits.argmax(1) == y).float().mean().item()\n",
    "    print(f\"[Sanity] val batch loss={loss.item():.4f}  acc@1={acc1*100:.2f}%\")\n",
    "except Exception as e:\n",
    "    print(\"[Sanity] Skipped:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
